{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 분석  \n",
    "## NLP이냐 텍스트 분석이냐?  \n",
    "* NLP와 텍스트분석  \n",
    "    1. NLP(National Language Processing) : 머신이 인간의 언어를 이해하고 해석하는 데 중점.  \n",
    "    2. 텍스트 마이닝(=텍스트 분석) : 비정형 텍스트에서 의미 있는 정보를 추출하는 것에 중점.  \n",
    "    3. **NLP는 텍스트 분석을 향상하게 하는 기반 기술**  \n",
    "    <br><br>\n",
    "    \n",
    "* 텍스트 마이닝 영역  \n",
    "    1. 텍스트 분류(Text Classification, Text Categorization)  \n",
    "        - 문서가 특정 분류 또는 카테고리에 속하는 것을 예측하는 기법.  \n",
    "        - 지도학습 적용.  \n",
    "        - 특정 신문 기사 내용이 어떤 카테고리에 속하는지, 스팸 메일 검출.  \n",
    "    2. 감성 분석(Sentiment Analysis)  \n",
    "        - 텍스트의 감정/판단/믿음/의견/기분 등의 주관적인 요소를 분석하는 기법.  \n",
    "        - 지도학습, 비지도학습.  \n",
    "        - 소셜 미디어 감정 분석, 영화/제품에 대한 리뷰, 여론조사 의견 분석 등.  \n",
    "    3. 텍스트 요약(Summarization)  \n",
    "        - 토픽 모델링(Topic Modeling)  \n",
    "    4. 텍스트 군집화(Clustering)와 유사도 측정  \n",
    "        - 비슷한 유형의 문서를 군집화.  \n",
    "        - 문서들간의 유사도를 측정 후 비슷한 문서끼리 군집화.  \n",
    "        \n",
    "<br><br>\n",
    "\n",
    "## 텍스트 분석 이해  \n",
    "* 텍스트 분석 : 비정형 데이터인 텍스트를 분석하는 것.  \n",
    "* 피처 벡터화(Feature Vectorization)  \n",
    "    1. 피처 추출(Feature Extraction)이라고도 함.  \n",
    "    2. 텍스트를 word 기반의 다수의 피처로 추출 -> 단어 빈도수와 같은 숫자 값 부여 -> 텍스트가 단어의 조합인 벡터값으로 표현됨.  \n",
    "    3. 텍스트를 벡터화 시키는 이유 : 머신러닝은 숫자형의 피처 기반 데이터만 입력받을 수 있기 때문.  \n",
    "    4. 피처 벡터화 하는 방법으로는 BOW(Bag of Words), Word2Vec 방법이 있음.  \n",
    "    \n",
    "<br>  \n",
    "\n",
    "### 텍스트 분석 수행 프로세스  \n",
    "\n",
    "    1. 텍스트 사전 준비작업(텍스트 전처리)    \n",
    "        - 텍스트를 피처로 만들기 전에 수행하는 작업.  \n",
    "        - 클렌징, 대/소문자 변경, 특수문자 삭제, 토큰화 작업, 의미 없는 단어 제거, 어근 추출 등.  \n",
    "    2. 피처 벡터와/추출    \n",
    "        - 가공된 텍스트에서 피처를 추출하고 벡터값을 할당.  \n",
    "        - BOW, Word2Vec  \n",
    "    3. ML 모델 수립 및 학습/예측/평가  \n",
    "        - 피처 벡터화된 데이터 세트에 ML 모델 적용.  \n",
    "        \n",
    "<br>\n",
    "\n",
    "### 파이썬 기반의 NLP, 텍스트 분석 패키지  \n",
    "    1. NLTK(Natural Language Toolkit for Python)  \n",
    "        - 파이썬의 가장 대표적인 NLP 패키지.  \n",
    "        - 수행 속도 측면에서 아쉬운 부분이 있어 실제 대량의 데이터 기반에서는 제대로 활용되지 못하고 있음.  \n",
    "    2. Gensim  \n",
    "        - 토픽 모델링 분야에서 가장 두각을 나타내는 패키지.  \n",
    "        - SpaCy와 함께 가장 많이 사용되는 NLP 패키지.  \n",
    "    3. SpaCy  \n",
    "        - 뛰어난 수행 성능으로 최근 가장 주목 받는 NLP 패키지.  \n",
    "        \n",
    "<br>\n",
    "\n",
    "## 텍스트 사전 준비 작업(텍스트 전처리) - 텍스트 정규화  \n",
    "* 텍스트 정규화  \n",
    "    1. 다양한 텍스트 데이터의 사전 작업을 수행하는 것.  \n",
    "    2. 클렌징(Cleansing), 토큰화(Tokenization), 필터링/스톱 워드 제거/철자 수정, Stemming, Lemmatization.  \n",
    "    \n",
    "    \n",
    "### 클렌징  \n",
    "    * 텍스트 분석에서 방해가 되는 불필요한 문자, 기호 등을 사전에 제거하는 작업.  \n",
    "\n",
    "    \n",
    "### 텍스트 토큰화  \n",
    "\n",
    "텍스트 토큰화는 **문장 토큰화**와 **단어 토큰화**로 나눌 수 있음.  \n",
    "\n",
    "#### 문장 토큰화(sentence tokenization)    \n",
    "    1. 문장의 마지막을 뜻하는 기호(문장 마침표(.), 개행문자(\\n))에 따라 분리하는 것.  \n",
    "    2. 정규 표현식에 따른 문장 토큰화도 가능함.  \n",
    "    3. 일반적으로 문장 토큰화는 각 문장이 가지는 시맨틱적인 의미가 중요한 요소로 사용될 때 사용함.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# NTLK의 sent_tokenize를 이용해 토큰화 수행\n",
    "# 3개의 문장으로 이루어진 텍스트 문서를 문장으로 각각 분리하는 예제\n",
    "from nltk import sent_tokenize\n",
    "import nltk\n",
    "# nltk.download('punkt') : 마침표, 개행 문자등의 데이터 세트 다운\n",
    "nltk.download('punkt')\n",
    "\n",
    "text_sample = \"The Matrix is everywhere its all around us, here even in this room. \\\n",
    "               You can see it out your window or on your television. \\\n",
    "               You feel it when you go to work, or go to church or pay your taxes.\"\n",
    "sentences = sent_tokenize(text=text_sample)\n",
    "\n",
    "# sent_tokenize()는 각각의 문장으로 구성된 list 객체를 반환함.\n",
    "print(type(sentences), len(sentences))\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 단어 토큰화  \n",
    "    1. 문장을 단어로 토큰화.  \n",
    "    2. 공백, 콤마(,), 마침표(.), 개행문자 등으로 분리.  \n",
    "    3. 정규 표현식을 이용해 다양한 유형으로 토큰화를 수행할 수 있음.  \n",
    "    4. 단어의 순서가 중요하지 않은 경우 문장 토큰화를 사용하지 않고 단어 토큰화만 사용함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 15\n",
      "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'roo', '.']\n"
     ]
    }
   ],
   "source": [
    "# NTLK의 word_tokenize()를 이용해 단어로 토큰화\n",
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = \"The Matrix is everywhere its all around us, here even in this roo.\"\n",
    "words = word_tokenize(sentence)\n",
    "print(type(words), len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "# 여러 개의 문장으로 된 입력 데이터를 문장별로 단어 토큰화하게 만드는 함수 생성\n",
    "def tokenize_text(text):\n",
    "    # 문장별로 분리 토큰\n",
    "    sentences = sent_tokenize(text=text)\n",
    "    \n",
    "    # 분리된 문장별 단어 토큰화\n",
    "    words = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "    return words\n",
    "\n",
    "text_sample = \"The Matrix is everywhere its all around us, here even in this room. \\\n",
    "               You can see it out your window or on your television. \\\n",
    "               You feel it when you go to work, or go to church or pay your taxes.\"\n",
    "\n",
    "# 여러 문장에 대해 문장별 단어 토큰화 수행.\n",
    "words = tokenize_text(text_sample)\n",
    "print(type(words), len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문장을 단어 토큰화할 경우 문맥의 의미는 무시될 수밖에 없음. -> 이를 해결하는 방법 : **n-gram**  \n",
    "* n-gram  \n",
    "    1. 연속된 n개의 단어를 하나의 토큰화 단위로 분리해 내는 것.  \n",
    "    2. ex) \"Agent Smith knocks the door\"를 2-gram으로 만들면  \n",
    "        (Agent, Smith), (Smith, knocks), (knocks, the), (the, door)  \n",
    "\n",
    "<br>\n",
    "\n",
    "### 스톱 워드 제거  \n",
    "* 스톱 워드(Stop word)  \n",
    "    1. 분석에 큰 의미가 없는 단어.  \n",
    "    2. 문맥적으로 큰 의미가 없는 단어.  \n",
    "    3. ex) is, the, a, will 등  \n",
    "\n",
    "**스톱 워드의 경우 문법적인 특성으로 텍스트에 빈번하게 나타남. -> 빈번하게 나타나게 되면 중요한 단어로 인지될 수 있음. -> 사전 제거 중요!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK의 stopwords 목록 내려받기\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 stop words 개수 :  179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    }
   ],
   "source": [
    "print(\"영어 stop words 개수 : \", len(nltk.corpus.stopwords.words('english')))\n",
    "print(nltk.corpus.stopwords.words('english')[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "all_tokens = []\n",
    "# 위 예제에서 3개의 문장별로 얻은 word_tokens list에 대해 스톱 워드를 제거하는 반복문\n",
    "for sentence in words:\n",
    "    filtered_words = []\n",
    "    # 개별 문장별로 토큰화된 문장 list에 대해 스톱 워드를 제거하는 반복문\n",
    "    for word in sentence:\n",
    "        # 소문자로 모두 변환.\n",
    "        word = word.lower()\n",
    "        # 토큰화된 개별 단어가 스톱 워드의 단어에 포함되지 않으면 word_tokens에 추가\n",
    "        if word not in stopwords:\n",
    "            filtered_words.append(word)\n",
    "    all_tokens.append(filtered_words)\n",
    "    \n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming과 Lemmatization  \n",
    "* Stemming과 Lemmatization  \n",
    "    1. **문법적 또는 의미적으로 변화하는 단어의 원형을 찾는 것.**    \n",
    "    2. Lemmatization이 Stemming보다 정교하며 의미론적인 기반에서 단어의 원형을 찾음. -> 변환 시 Lemmatization이 더 오래 걸림.  \n",
    "    3. Stemming은 더 단순화된 방법을 적용해 원래 단어에서 일부 철자가 훼손된 어근 단어를 추출하는 경향이 있음.  \n",
    "* NLTK의 Stemmer  \n",
    "    1. Porter, Lancaster, Snowball Stemmer가 있음.  \n",
    "    2. Lemmatization을 위해서는 WordNetLemmatizer를 제공함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "amus amus amus\n",
      "happy happiest\n",
      "fant fanciest\n"
     ]
    }
   ],
   "source": [
    "# NLTK의 LancasterStemmer를 이용해 stemmer 사용하기\n",
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "print(stemmer.stem('working'), stemmer.stem('works'), stemmer.stem('worked'))\n",
    "print(stemmer.stem('amusing'), stemmer.stem('amuses'), stemmer.stem('amused'))\n",
    "print(stemmer.stem('happier'), stemmer.stem('happiest'))\n",
    "print(stemmer.stem('fancier'), stemmer.stem('fanciest'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amuse amuse amuse\n",
      "happy happy\n",
      "fancy fancy\n"
     ]
    }
   ],
   "source": [
    "# WordNetLemmatizer를 이용해 Lemmatization 수행하기\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "# Lemmatization은 보다 정확한 원형 단어 추출을 위해 단어의 '품사'를 입력해야 함.\n",
    "# 동사는 'v', 형용사는 'a'\n",
    "print(lemma.lemmatize('amusing', 'v'), lemma.lemmatize('amuses', 'v'), lemma.lemmatize('amused', 'v'))\n",
    "print(lemma.lemmatize('happier', 'a'), lemma.lemmatize('happiest', 'a'))\n",
    "print(lemma.lemmatize('fancier', 'a'), lemma.lemmatize('fanciest', 'a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words = BOW  \n",
    "<img src='https://i1.wp.com/hleecaster.com/wp-content/uploads/2020/01/bow01.jpg?w=1200'>\n",
    "\n",
    "* BOW(Bag of Words)  \n",
    "    1. 문서가 가지는 모든 단어를 문맥이나 순서를 무시하고 일괄적으로 단어에 대해 빈도값을 부여해 피처값을 추출하는 모델.  \n",
    "    <img src='https://cdn.analyticsvidhya.com/wp-content/uploads/2020/02/BoWBag-of-Words-model-2.png'>  \n",
    "\n",
    "    2. BOW의 피처 추출 방법  \n",
    "        - 각 개별 문장에 있는 모든 단어에서 중복을 제거하고 각 단어를 칼럼 형태로 나열.  \n",
    "        - 각 단어에 고유의 인덱스를 부여.  \n",
    "            ex) 'This' : 1, 'movie' : 2, 'is' : 3, ..., 'good' : 11  \n",
    "        - 개별 문장에서 해당 단어가 나타나는 횟수를 각 단어(단어 인덱스)에 기재함.  \n",
    "    3. BOW의 장점 : 쉼고 빠른 구축.  \n",
    "    4. BOW의 단점  \n",
    "        - 문맥 의미 반영 부족 : 단어의 순서를 고려하지 않기 때문에 의미가 무시됨. 이를 해결하기 위한 방법으로 n-gram 기법을 활용하지만 제한적인 부분에 그쳐 문맥적인 해석을 처리하는데 어려움이 있음.  \n",
    "        - 희소 행렬 문제 : 모든 문서의 단어의 총 개수를 수십~수만개임. 하지만 각 문장별 단어는 일부분이기 때문에 대부분의 데이터가 0으로 채워지게 됨. 이렇게 되면 대부분의 값이 0으로 채워지는 희소 행렬 벡터가 되는데 이는 ML 알고리즘의 수행 시간과 예측 성능을 떨어뜨림.  \n",
    "        \n",
    "<br>\n",
    "\n",
    "### BOW 피처 벡터화  \n",
    "* BOW 피처 벡터화 방법 : 카운트 기반의 벡터화, TF-IDF(Term Frequency - Inverse Document Frequency) 기반의 벡터화.  \n",
    "* 카운트 벡터화  \n",
    "    1. 단어 피처의 값을 부여할 때 각 문서에서 해당 단어가 나타나는 횟수(Count)를 부여하는 경우.  \n",
    "    2. 카운트값이 높을수록 중요한 단어로 인식됨.  \n",
    "    3. 카운트만 부여할 경우, 문서의 특징을 나타낸다기 보다 문장에서 자주 사용될 수밖에 없는 단어가 높은 값을 갖게됨.  \n",
    "* TF-IDF 벡터화  \n",
    "    1. 카운트 벡터화에서 자주 사용될 수밖에 없는 단어가 높은 값을 갖게 되는 문제점을 보완한 방법.  \n",
    "    2. 개별 문장에서 자주 나타나는 단어에는 높은 가중치를 주고, 모든 문서에서 자주 나타나는 단어에 대해서는 페널티를 줌.  \n",
    "    3. 텍스트가 길고 문서의 개수가 많은 경우 TF-IDF가 카운트 벡터화를 사용하는 것보다 훨씬 성능이 좋음.  \n",
    "    \n",
    "<br>\n",
    "\n",
    "### BOW 벡터화를 위한 희소 행렬  \n",
    "* 피처 벡터화의 문제 : 희소 행렬  \n",
    "    1. 피처 벡터화를 수행하면 **많은 피처 칼럼**을 만들 수밖에 없음.  \n",
    "    2. 피처 벡터화 수행 후 대규모의 행렬이 생성되지만 레코드의 각 문서가 가지는 단어 수는 제한적이기 때문에 행렬의 대부분의 값이 0이 됨.  \n",
    "    3. 이렇게 대부분의 값이 0인 행렬을 **희소 행렬**이라 함.  \n",
    "* 희소 행렬의 문제점  \n",
    "    1. 메모리 공간이 많이 필요함.  \n",
    "    2. 연산 시 데이터 엑세스를 위한 시간이 많이 소모됨.\n",
    "    \n",
    "**따라서 우리는 이러한 희소 행렬을 물리적으로 적은 메모리 공간을 차지할 수 있도록 반환해야 함.**\n",
    "\n",
    "* 희소 행렬의 문제점을 보완하기 위한 방법 : **COO 형식, CSR 형식**  \n",
    "* COO 형식보다는 CSR 형식이 더 많이 쓰임.  \n",
    "\n",
    "<br>\n",
    "\n",
    "### 희소 행렬 - COO 형식  \n",
    "    1. 0이 아닌 데이터만 별도의 데이터 배열에 저장하고, 그 데이터가 가리키는 행과 열의 위치를 별도의 배열로 저장하는 방식.  \n",
    "        만약 피처 벡터화 후 데이터가 [[3, 0, 1], [0, 2, 0]] 이라면  \n",
    "        0이 아닌 데이터는 [3, 1, 2]이고, 위치를 (row, col)로 표시하면  \n",
    "        (0, 0), (0, 2), (1,1)이 됨.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 넘파이로 [[3, 0, 1], [0, 2, 0]] 배열 만들기\n",
    "import numpy as np\n",
    "\n",
    "dense = np.array( [[3, 0, 1], [0, 2, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사이파이의 sparse 패키지에서 coo_matrix 클래스를 사용하여 희소 행렬 변환을 COO 형식을 수행하기\n",
    "from scipy import sparse\n",
    "\n",
    "# 0이 아닌 데이터 추출\n",
    "data = np.array([3, 1, 2])\n",
    "\n",
    "# 행 위치와 열 위치를 각각 배열로 생성\n",
    "row_pos = np.array([0, 0, 1])\n",
    "col_pos = np.array([0, 2, 1])\n",
    "\n",
    "# spars 패키지의 coo_matrix를 이용해 COO 형식으로 희소 행렬 생성\n",
    "sparse_coo = sparse.coo_matrix((data, (row_pos, col_pos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 1],\n",
       "       [0, 2, 0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_coo.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 희소 행렬 - CSR 형식  \n",
    "* CSR(Compressed Sparse Row) 형식은 COO 형식이 행과 열의 위치를 나타내기 위해 반복적인 위치 데이터를 사용해야 하는 문제점을 해결한 방식.  \n",
    "<br>\n",
    "\n",
    "[[0, 0, 1, 0, 0, 5], [1, 4, 0, 3, 2, 5], [0, 6, 0, 3, 0, 0], [2, 0, 0, 0, 0, 0], [0, 0, 0, 7, 0, 8], [1, 0, 0, 0, 0, 0]] 2차원 데이터를 COO 형식으로 나타내면  \n",
    "데이터 배열은 [1, 5, 1, 4, 3, 2, 5, 6, 3, 2, 7, 8, 1]이고, 행 위치 배열은 [0, 0, 1, 1, 1, 1, 1, 2, 2, 3, 4, 4, 5], 열 위치 배열은 [2, 5, 0, 1, 3, 4, 5, 1, 3, 0, 3 ,5, 0]이 됨.  \n",
    "행 위치 배열과 열 위치 배열을 보면 숫자가 반복적으로 나타남을 알 수 있음. (예를 들어 행 위치 배열에선 0이 2번, 1이 5번, 2가 2번, 4가 2번)  \n",
    "이는 많은 메모리를 차지하고 연산이 느려짐.  \n",
    "따라서 **CSR은 행 위치 배열이 반복적인 값을 갖지 않도록 시작 위치만 다시 별도의 위치 배열로 가지는 변환 방식임.**  \n",
    "따라서 행 위치 배열을 CSR로 다시 나타내면 [0, 2, 7, 9, 10, 12]가 됨.(새로운 숫자가 나타나는 시작 위치 인덱스로 표현)\n",
    "이로 인해 메모리가 적게 들고 빠른 연산이 가능해짐.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n",
      "CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# CSR 방식 변환은 사이파이의 csr_matrix 클래스를 이용해 할 수 있음.\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "\n",
    "dense2 = np.array([[0, 0, 1, 0, 0, 5], \n",
    "                   [1, 4, 0, 3, 2, 5], \n",
    "                   [0, 6, 0, 3, 0, 0], \n",
    "                   [2, 0, 0, 0, 0, 0], \n",
    "                   [0, 0, 0, 7, 0, 8], \n",
    "                   [1, 0, 0, 0, 0, 0]])\n",
    "\n",
    "\n",
    "# 0이 아닌 데이터 추출\n",
    "data2 = np.array([1, 5, 1, 4, 3, 2, 5, 6, 3, 2, 7, 8, 1])\n",
    "\n",
    "# 행 위치와 열 위치를 각각 array로 생성\n",
    "row_pos = np.array([0, 0, 1, 1, 1, 1, 1, 2, 2, 3, 4, 4, 5])\n",
    "col_pos = np.array([2, 5, 0, 1, 3, 4, 5, 1, 3, 0, 3, 5, 0])\n",
    "\n",
    "# COO 형식으로 변환\n",
    "sparse_coo = sparse.coo_matrix((data2, (row_pos, col_pos)))\n",
    "\n",
    "# 행 위치 배열의 고유한 값의 시작 위치 인덱스를 배열로 생성\n",
    "row_pos_ind = np.array([0, 2, 7, 9, 10, 12, 13])\n",
    "\n",
    "# CSR 형식으로 변환\n",
    "sparse_csr = sparse.csr_matrix((data2, col_pos, row_pos_ind))\n",
    "\n",
    "print(\"COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\")\n",
    "print(sparse_coo.toarray())\n",
    "print(\"CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\")\n",
    "print(sparse_csr.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 분류 실습 - 20 뉴스그룹 분류  \n",
    "- 사이킷런이 내부에 가지고 있는 예제 데이터인 20 뉴스그룹 데이터셋을 이용.  \n",
    "- 로지스틱 회귀를 이용해 분류 수행.  \n",
    "<br>\n",
    "\n",
    "### 텍스트 정규화  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# subset='all'이면 모든 데이터 다 가져옴. subset='target'이면 타겟 데이터만\n",
    "news_data = fetch_20newsgroups(subset='all', random_state=156)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n"
     ]
    }
   ],
   "source": [
    "# key값 확인\n",
    "print(news_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target 클래스의 값과 분포도 \n",
      " 0     799\n",
      "1     973\n",
      "2     985\n",
      "3     982\n",
      "4     963\n",
      "5     988\n",
      "6     975\n",
      "7     990\n",
      "8     996\n",
      "9     994\n",
      "10    999\n",
      "11    991\n",
      "12    984\n",
      "13    990\n",
      "14    987\n",
      "15    997\n",
      "16    910\n",
      "17    940\n",
      "18    775\n",
      "19    628\n",
      "dtype: int64\n",
      "target 클래스의 이름들 \n",
      " ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "# target 확인\n",
    "import pandas as pd\n",
    "\n",
    "print(\"target 클래스의 값과 분포도 \\n\", pd.Series(news_data.target).value_counts().sort_index())\n",
    "print(\"target 클래스의 이름들 \\n\", news_data.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: egreen@east.sun.com (Ed Green - Pixel Cruncher)\n",
      "Subject: Re: Observation re: helmets\n",
      "Organization: Sun Microsystems, RTP, NC\n",
      "Lines: 21\n",
      "Distribution: world\n",
      "Reply-To: egreen@east.sun.com\n",
      "NNTP-Posting-Host: laser.east.sun.com\n",
      "\n",
      "In article 211353@mavenry.altcit.eskimo.com, maven@mavenry.altcit.eskimo.com (Norman Hamer) writes:\n",
      "> \n",
      "> The question for the day is re: passenger helmets, if you don't know for \n",
      ">certain who's gonna ride with you (like say you meet them at a .... church \n",
      ">meeting, yeah, that's the ticket)... What are some guidelines? Should I just \n",
      ">pick up another shoei in my size to have a backup helmet (XL), or should I \n",
      ">maybe get an inexpensive one of a smaller size to accomodate my likely \n",
      ">passenger? \n",
      "\n",
      "If your primary concern is protecting the passenger in the event of a\n",
      "crash, have him or her fitted for a helmet that is their size.  If your\n",
      "primary concern is complying with stupid helmet laws, carry a real big\n",
      "spare (you can put a big or small head in a big helmet, but not in a\n",
      "small one).\n",
      "\n",
      "---\n",
      "Ed Green, former Ninjaite |I was drinking last night with a biker,\n",
      "  Ed.Green@East.Sun.COM   |and I showed him a picture of you.  I said,\n",
      "DoD #0111  (919)460-8302  |\"Go on, get to know her, you'll like her!\"\n",
      " (The Grateful Dead) -->  |It seemed like the least I could do...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 데이터 확인\n",
    "print(news_data.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터 크기 11314, 테스트 데이터 크기 7532\n"
     ]
    }
   ],
   "source": [
    "# 우리의 목표는 순수한 텍스트 데이터를 이용해 분류하는 것이므로 헤더 등을 삭제해야 함.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# subset='train'으로 학습용 데이터만 추출, remove=('headers', 'footers', 'quotes')로 내용만 추출\n",
    "train_news = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), random_state=156)\n",
    "X_train = train_news.data\n",
    "y_train = train_news.target\n",
    "\n",
    "# subset='test'로 테스트 데이터만 추출, remove=('headers', 'footers', 'quotes')로 내용만 추출\n",
    "test_news = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'), random_state=156)\n",
    "X_test = test_news.data\n",
    "y_test = test_news.target\n",
    "\n",
    "print(\"학습 데이터 크기 {0}, 테스트 데이터 크기 {1}\".format(len(train_news.data), len(test_news.data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 피처 벡터화 변환과 머신러닝 모델 학습/예측/평가  \n",
    "- CountVectorizer를 이용해 학습 데이터의 텍스트와 테스트 데이터의 텍스트를 피처 벡터화.  \n",
    "    **주의할 점**  \n",
    "    1. 테스트 데이터의 텍스트를 피처 벡터화할 때는 반드시 학습 데이터에서 사용한 CountVectorizer 객체를 사용해야 함.  \n",
    "    2. 테스트 데이터에서는 fit_transform() 사용해서는 안됨.  \n",
    "    -> 1번과 2번을 지키지 않을 경우 학습 데이터와 테스트 데이터의 피처 개수가 달라짐.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터 텍스트의 CountVectorizer Shape :  (11314, 101631)\n"
     ]
    }
   ],
   "source": [
    "# CountVectorizer를 이용해 피처 벡터화\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Count Vectoriization으로 피처 벡터화 변환 수행.\n",
    "cnt_vect = CountVectorizer()\n",
    "cnt_vect.fit(X_train)\n",
    "X_train_cnt_vect = cnt_vect.transform(X_train)\n",
    "\n",
    "# 학습 데이터로 fit()된 CountVectorizer를 이용해 테스트 데이터를 피처 벡터화 변환 수행.\n",
    "X_test_cnt_vect = cnt_vect.transform(X_test)\n",
    "\n",
    "print(\"학습 데이터 텍스트의 CountVectorizer Shape : \", X_train_cnt_vect.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorized Logistic Regression의 예측 정확도 : 0.617\n"
     ]
    }
   ],
   "source": [
    "# 피처 벡터화된 데이터에 로지스틱 회귀를 적용해 뉴스 그룹에 대한 분류 예측\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# LogisticRetression을 이용해 학습/예측/평가 수행\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(X_train_cnt_vect, y_train)\n",
    "pred = lr_clf.predict(X_test_cnt_vect)\n",
    "print(\"CountVectorized Logistic Regression의 예측 정확도 : {0:.3f}\".format(accuracy_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TF-IDF를 이용한 학습 데이터와 테스트 데이터 피처 벡터화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Logistic Regression의 예측 정확도는 0.678\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TF-IDF 벡터화를 적용해 학습 데이터 세트와 테스트 데이터 세트 변환.\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "tfidf_vect.fit(X_train)\n",
    "X_train_tfidf_vect = tfidf_vect.transform(X_train)\n",
    "X_test_tfidf_vect = tfidf_vect.transform(X_test)\n",
    "\n",
    "# LogisticRegression을 이용해 학습/예측/평가 수행.\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(X_train_tfidf_vect, y_train)\n",
    "pred = lr_clf.predict(X_test_tfidf_vect)\n",
    "print(\"TF-IDF Logistic Regression의 예측 정확도는 {0:.3f}\".format(accuracy_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 텍스트 분석에서 예측 성능을 향상시키는 방법.  \n",
    "    1. 최적의 ML 모델 사용.  \n",
    "    2. 최상의 피처 전처리 수행.  \n",
    "    \n",
    "* 이번에는 TF-IDF의 다양한 파라미터를 적용해 볼 것임.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Vectorized Logistic Regression의 예측 정확도는 0.690\n"
     ]
    }
   ],
   "source": [
    "# stop words 필터링을 추가하고 ngram을 기본(1, 1)에서 (1, 2)로 변경해 피처 벡터화 적용.\n",
    "tfidf_vect = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), max_df=300)\n",
    "tfidf_vect.fit(X_train)\n",
    "X_train_tfidf_vect = tfidf_vect.transform(X_train)\n",
    "X_test_tfidf_vect = tfidf_vect.transform(X_test)\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(X_train_tfidf_vect, y_train)\n",
    "pred = lr_clf.predict(X_test_tfidf_vect)\n",
    "print(\"TF-IDF Vectorized Logistic Regression의 예측 정확도는 {0:.3f}\".format(accuracy_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. GridSearch를 이용해 로지스틱 회귀 모델의 최적 하이퍼 파라미터를 찾음.  \n",
    "2. 로지스틱 회귀의 파라미터 중 C(alpha의 역수)만 변경해 최적의 파라미터를 찾음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:  3.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression best C parameter :  {'C': 10}\n",
      "TF-IDF Vectorized Logistic Regression의 예측 정확도는 0.704\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 최적 C값 도출 튜닝 수행. CV는 3 폴드 세트로 설정.\n",
    "params = {'C' : [0.01, 0.1, 1, 5, 10]}\n",
    "grid_cv_lr = GridSearchCV(lr_clf, param_grid=params, cv=3, scoring='accuracy', verbose=1)\n",
    "grid_cv_lr.fit(X_train_tfidf_vect, y_train)\n",
    "print(\"Logistic Regression best C parameter : \", grid_cv_lr.best_params_)\n",
    "\n",
    "# 최적 C값으로 학습된 grid_cv로 예측 및 정확도 평가.\n",
    "pred = grid_cv_lr.predict(X_test_tfidf_vect)\n",
    "print(\"TF-IDF Vectorized Logistic Regression의 예측 정확도는 {0:.3f}\".format(accuracy_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사이킷런 파이프라인(Pipeline) 사용 및 GridSearchCV와의 결합  \n",
    "* 머신러닝에서 Pipeline의 의미  \n",
    "     **\"데이터의 가공, 변환 등의 전처리와 알고리즘 적용을 마치 수도관(Pipe)에서 물 흐르듯 한꺼번에 스트림 기반으로 처리한다.\"**  \n",
    "<br>\n",
    "\n",
    "* 사이킷런의 Pipeline 클래스  \n",
    "    1. 피처 벡터화와 ML 알고리즘 학습/예측을 위한 코드 작성을 한 번에 진행.  \n",
    "    2. 피처 벡터화뿐만 아니라 모든 데이터 전처리 작업과 Estimator를 결합할 수 있음.(스케일링, 정규화 PCA, Estimator 등)  \n",
    "    3. 더 직관적인 ML 모델 코드 생성.  \n",
    "    4. 수행 시간 절약.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline을 통한 Logistic Regression의 예측 정확도는 0.704\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# TfidfVetorizer 객체를 tfidf_vect로, LogisticRegression 객체를 lr_clf로 생성하는 Pipeline 생성\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf_vect', TfidfVectorizer(stop_words='english', ngram_range=(1, 2), max_df=300)), \n",
    "    ('lr_clf', LogisticRegression(C=10))\n",
    "])\n",
    "\n",
    "# 별도의 TfidfVectorizer 객체의 fit(), transform()과 LogisticRegression의 fit(), predict()가 필요 없음.\n",
    "# pipeline의 fit()과 predict()만으로 한꺼번에 피처 벡터화와 ML 학습/예측이 가능.\n",
    "pipeline.fit(X_train, y_train)\n",
    "pred = pipeline.predict(X_test)\n",
    "print(\"Pipeline을 통한 Logistic Regression의 예측 정확도는 {0:.3f}\".format(accuracy_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 사이킷런은 GridSearchCV 클래스에서 Estimator 파라미터 대신 Pipeline 객체를 입력하면 Pipeline 기반에서도 하이퍼 파라미터 튜닝을 GridSerchCV 방식으로 진행할 수 있게 함.  \n",
    "* 이를 통해 우리는 피처 벡터화와 ML 알고리즘의 하이퍼 파라미터를 GridSearchCV를 이용해 한번에 최적화할 수 있음.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "[Parallel(n_jobs=1)]: Done  81 out of  81 | elapsed: 27.5min finished\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'best_scores_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-00dbda06017f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mgrid_cv_pipe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mgrid_cv_pipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid_cv_pipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrid_cv_pipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_scores_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid_cv_pipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'best_scores_'"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf_vect', TfidfVectorizer(stop_words='english')), \n",
    "    ('lr_clf', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Pipeline에 기술된 각각의 객체 변수에 언더바(_) 2개를 연달아 붙여 GridSearchCV에 사용될 파라미터/하이퍼 파라미터 이름과 값을 설정.\n",
    "params = {'tfidf_vect__ngram_range' : [(1, 1), (1, 2), (1, 3)], \n",
    "          'tfidf_vect__max_df' : [100, 300, 700], \n",
    "          'lr_clf__C' : [1, 5, 10]}\n",
    "\n",
    "# GridSearchCV의 생성자에 Estimator가 아닌 Pipeline 객체 입력\n",
    "grid_cv_pipe = GridSearchCV(pipeline, param_grid=params, cv=3, scoring='accuracy', verbose=1)\n",
    "grid_cv_pipe.fit(X_train, y_train)\n",
    "print(grid_cv_pipe.best_params_, grid_cv_pipe.best_scores_)\n",
    "\n",
    "pred = grid_cv_pipe.predict(X_test)\n",
    "print(\"Pipeline을 통한 Logistic Regression의 예측 정확도는 {0:.3f}\".format(accuracy_score(y_test, pred)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 감성 분석  \n",
    "### 감성 분석 소개  \n",
    "    1. 감성 분석(Sentiment Analysis)은 문서의 주관적인 감성/의견/감정/기분 등을 파악하기 위한 방법.  \n",
    "    2. 소셜 미디어, 여론조사, 온라인 리뷰, 피드백 등 다양한 분야에서 활용됨.  \n",
    "    3. 감성(Sentiment) 수치를 계산하는 방법을 이용 -> 감성 지수는 긍정 감성 지수와 부정 감성 지수로 구성되고 이 지수를 합산해 긍정 감성 또는 부정 감성을 결정함.  \n",
    "    4. 감성 분석은 지도학습과 비지도학습 방식으로 나눌 수 있음.  \n",
    "    \n",
    "<br>\n",
    "\n",
    "### 지도학습 기반 감성 분석 실습 - IMDB 영화평  \n",
    "    영화평의 텍스트를 분석해 감성 분석 결과가 긍정 또는 부정인지를 예측하는 모델 만들어 보기  \n",
    "    \n",
    "https://www.kaggle.com/c/word2vec-nlp-tutorial/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# header=0 : 첫번째 행이 칼럼 이름일 경우 header=0으로 표시\n",
    "# quoting=3 : 큰따음표 무시\n",
    "review_df = pd.read_csv('C:\\\\Users\\\\eh063\\\\Desktop\\\\은하\\\\BOAZ\\\\분석 BASE\\\\ML\\\\파이썬 머신러닝 완벽 가이드\\\\Chapter8\\\\word2vec-nlp-tutorial\\\\labeledTrainData.tsv\\\\labeledTrainData.tsv', header=0, sep='\\t', quoting=3)\n",
    "review_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        * id : 각 데이터의 id  \n",
    "        * sentiment : 영화평의 sentiment 결과값(Target Label). 0은 부정, 1은 긍정.  \n",
    "        * review : 영화평의 텍스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"With all this stuff going down at the moment with MJ i've started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay.<br /><br />Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.<br /><br />The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci's character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ's music.<br /><br />Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.<br /><br />Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ's bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i've gave this subject....hmmm well i don't know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\"\n"
     ]
    }
   ],
   "source": [
    "print(review_df['review'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review 칼럼에서 <br /> 태그를 공백으로 바꿈.\n",
    "import re\n",
    "\n",
    "review_df['review'] = review_df['review'].str.replace('<br />', ' ')\n",
    "\n",
    "# review 칼럼에서 영어가 아닌 숫자, 특수문자 모두 공백으로 변환.\n",
    "review_df['review'] = review_df['review'].apply(lambda x : re.sub(\"[^a-zA-Z]\", \" \", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " With all this stuff going down at the moment with MJ i ve started listening to his music  watching the odd documentary here and there  watched The Wiz and watched Moonwalker again  Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent  Moonwalker is part biography  part feature film which i remember going to see at the cinema when it was originally released  Some of it has subtle messages about MJ s feeling towards the press and also the obvious message of drugs are bad m kay   Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring  Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him   The actual feature film bit when it finally starts is only on for    minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord  Why he wants MJ dead so bad is beyond me  Because MJ overheard his plans  Nah  Joe Pesci s character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno  maybe he just hates MJ s music   Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence  Also  the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene   Bottom line  this movie is for people who like MJ on one level or another  which i think is most people   If not  then stay away  It does try and give off a wholesome message and ironically MJ s bestest buddy in this movie is a girl  Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty  Well  with all the attention i ve gave this subject    hmmm well i don t know because people can be different behind closed doors  i know this for a fact  He is either an extremely nice but stupid guy or one of the most sickest liars  I hope he is not the latter  \n"
     ]
    }
   ],
   "source": [
    "print(review_df['review'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17500, 1), (7500, 1))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target값과 feature값 구분\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class_df = review_df['sentiment']\n",
    "feature_df = review_df.drop(['id', 'sentiment'], axis=1, inplace=False)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_df, class_df, test_size=0.3, random_state=156)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 정확도는 0.8861, ROC-AUC는 0.9503\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# 스톱 워드는 English, filtering, ngram은 (1, 2)로 설정해 CountVectorization 수행.\n",
    "# LogisticRegression의 C는 10으로 설정.\n",
    "pipeline = Pipeline([\n",
    "    ('cnt_vect', CountVectorizer(stop_words='english', ngram_range=(1, 2))), \n",
    "    ('lr_clf', LogisticRegression(C=10))])\n",
    "\n",
    "# Pipeline 객체를 이용해 fit(), predict()로 학습/예측 수행. predict_proba()는 roc_auc 때문에 수행.\n",
    "pipeline.fit(X_train['review'], y_train)\n",
    "pred = pipeline.predict(X_test['review'])\n",
    "pred_probs = pipeline.predict_proba(X_test['review'])[:, 1]\n",
    "\n",
    "print(\"예측 정확도는 {0:.4f}, ROC-AUC는 {1:.4f}\".format(accuracy_score(y_test, pred), \n",
    "                                                        roc_auc_score(y_test, pred_probs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 정확도는 0.8936, ROC-AUC는 0.9598\n"
     ]
    }
   ],
   "source": [
    "# 스톱 워드는 english, filtering, ngram은 (1, 2)로 설정해 TF-IDF 벡터화 수행.\n",
    "# LogisticRegression의 C는 10으로 설정.\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf_vect', TfidfVectorizer(stop_words='english', ngram_range=(1, 2))), \n",
    "    ('lr_clf', LogisticRegression(C=10))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train['review'], y_train)\n",
    "pred = pipeline.predict(X_test['review'])\n",
    "pred_probs = pipeline.predict_proba(X_test['review'])[:, 1]\n",
    "\n",
    "print(\"예측 정확도는 {0:.4f}, ROC-AUC는 {1:.4f}\".format(accuracy_score(y_test, pred), roc_auc_score(y_test, pred_probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 비지도학습 기반 감성 분석 소개  \n",
    "1. 비지도 감성 분석은 Lexicon을 기반으로 함.  \n",
    "    * Lexicon  \n",
    "        1) 감성만을 분석하기 위해 지원하는 감성 어휘 사전. 줄여서 감성 사전이라 함.  \n",
    "        2) 감성 사전은 긍정 감성 또는 부정 감성의 정도를 의미하는 수치를 가지고 있고 이를 감성 지수라고 함.  \n",
    "        3) 감성 지수는 단어의 위치, 주변 단어, 문맥, POS 등을 참고해 결정됨.  \n",
    "        4) 감성 사전을 구현한 대표적인 패키지는 NLTK 패키지임.  \n",
    "        \n",
    "2. NLP 패키지의 WordNet  \n",
    "    1) WordNet은 단순한 어휘 사전이 아닌 시맨틱 분석을 제공하는 방대한 영어 어휘 사전임.  \n",
    "        * 시맨틱(semantic) : 문맥상 의미  \n",
    "    2) WordNet은 어휘의 시맨틱 정보를 제공하기 위해 각각의 품사(명사, 동사, 형용사, 부사 등)로 구성된 개별 단어를 Synset(Sets of congnitive synonyms)이라는 개념을 이용해 표현함.  \n",
    "    3) Synset : 그 단어가 가지는 문맥, 시맨틱 정보를 제공하는 WordNet의 핵심 개념.  \n",
    "    \n",
    "3. NLTK 패키지는 예측 성능이 좋지 못함.  \n",
    "4. 대표적인 감성 사전  \n",
    "    1) SentiWordNet  \n",
    "        * WordNet의 Synset 개념을 감성 분석에 적용한 것.  \n",
    "        * WordNet의 Synset별로 3가지 감성 점수(긍정 감성 지수, 부정 감성 지수, 객관성 지수)를 할당함.  \n",
    "        * 문장별로 단어들의 긍정 감성 지수와 부정 감성 지수를 합산하여 최종 감성 지수를 계산하고 이에 기반해 감성이 긍정인지 부정인지 결정.  \n",
    "    2) VADER  \n",
    "        * 주로 소셜 미디어의 텍스트에 대한 감성 분석을 제공하기 위한 패키지.  \n",
    "        * 뛰어난 감성 분석 결과 제공.  \n",
    "        * 비교적 빠른 수행 시간 보장.  \n",
    "        \n",
    "<br>\n",
    "\n",
    "### SentiWordNet을 이용한 감성 분석  \n",
    "#### WordNet Synset과 SentiWordNet SentiSynset 클래스의 이해  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\alpino.zip.\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\omw.zip.\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\paradigms.zip.\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pl196x.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\problem_reports.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ptb.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Unzipping corpora\\pros_cons.zip.\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\rte.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\senseval.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\smultron.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\switchboard.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\webtext.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ycoe.zip.\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\rslp.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\book_grammars.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\large_grammars.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\perluniprops.zip.\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Unzipping corpora\\nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\porter_test.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\eh063\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\mwa_ppdb.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synsets() 반환 type :  <class 'list'>\n",
      "synsets() 반환 값 개수 :  18\n",
      "synsets() 반환 값 :  [Synset('present.n.01'), Synset('present.n.02'), Synset('present.n.03'), Synset('show.v.01'), Synset('present.v.02'), Synset('stage.v.01'), Synset('present.v.04'), Synset('present.v.05'), Synset('award.v.01'), Synset('give.v.08'), Synset('deliver.v.01'), Synset('introduce.v.01'), Synset('portray.v.04'), Synset('confront.v.03'), Synset('present.v.12'), Synset('salute.v.06'), Synset('present.a.01'), Synset('present.a.02')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "term = \"present\"\n",
    "\n",
    "# \"present\"라는 단어로 wordnet의 synsets 생성.\n",
    "synsets = wn.synsets(term)\n",
    "print(\"synsets() 반환 type : \", type(synsets))\n",
    "print(\"synsets() 반환 값 개수 : \", len(synsets))\n",
    "print(\"synsets() 반환 값 : \", synsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synset('present.n.01')에서  \n",
    "    - present는 의미  \n",
    "    - n은 명사 품사  \n",
    "    - 01 present가 명사로서 가지는 의미가 여러 가지라 이를 구분하는 인덱스  \n",
    "    \n",
    "Synset은 POS(품사), 정의, 부명제 등으로 시맨틱적인 요소를 표현할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Synset name :  present.n.01 #####\n",
      "POS :  noun.time\n",
      "Definition :  the period of time that is happening now; any continuous stretch of time including the moment of speech\n",
      "Lemmas :  ['present', 'nowadays']\n",
      "##### Synset name :  present.n.02 #####\n",
      "POS :  noun.possession\n",
      "Definition :  something presented as a gift\n",
      "Lemmas :  ['present']\n",
      "##### Synset name :  present.n.03 #####\n",
      "POS :  noun.communication\n",
      "Definition :  a verb tense that expresses actions or states at the time of speaking\n",
      "Lemmas :  ['present', 'present_tense']\n",
      "##### Synset name :  show.v.01 #####\n",
      "POS :  verb.perception\n",
      "Definition :  give an exhibition of to an interested audience\n",
      "Lemmas :  ['show', 'demo', 'exhibit', 'present', 'demonstrate']\n",
      "##### Synset name :  present.v.02 #####\n",
      "POS :  verb.communication\n",
      "Definition :  bring forward and present to the mind\n",
      "Lemmas :  ['present', 'represent', 'lay_out']\n",
      "##### Synset name :  stage.v.01 #####\n",
      "POS :  verb.creation\n",
      "Definition :  perform (a play), especially on a stage\n",
      "Lemmas :  ['stage', 'present', 'represent']\n",
      "##### Synset name :  present.v.04 #####\n",
      "POS :  verb.possession\n",
      "Definition :  hand over formally\n",
      "Lemmas :  ['present', 'submit']\n",
      "##### Synset name :  present.v.05 #####\n",
      "POS :  verb.stative\n",
      "Definition :  introduce\n",
      "Lemmas :  ['present', 'pose']\n",
      "##### Synset name :  award.v.01 #####\n",
      "POS :  verb.possession\n",
      "Definition :  give, especially as an honor or reward\n",
      "Lemmas :  ['award', 'present']\n",
      "##### Synset name :  give.v.08 #####\n",
      "POS :  verb.possession\n",
      "Definition :  give as a present; make a gift of\n",
      "Lemmas :  ['give', 'gift', 'present']\n",
      "##### Synset name :  deliver.v.01 #####\n",
      "POS :  verb.communication\n",
      "Definition :  deliver (a speech, oration, or idea)\n",
      "Lemmas :  ['deliver', 'present']\n",
      "##### Synset name :  introduce.v.01 #####\n",
      "POS :  verb.communication\n",
      "Definition :  cause to come to know personally\n",
      "Lemmas :  ['introduce', 'present', 'acquaint']\n",
      "##### Synset name :  portray.v.04 #####\n",
      "POS :  verb.creation\n",
      "Definition :  represent abstractly, for example in a painting, drawing, or sculpture\n",
      "Lemmas :  ['portray', 'present']\n",
      "##### Synset name :  confront.v.03 #####\n",
      "POS :  verb.communication\n",
      "Definition :  present somebody with something, usually to accuse or criticize\n",
      "Lemmas :  ['confront', 'face', 'present']\n",
      "##### Synset name :  present.v.12 #####\n",
      "POS :  verb.communication\n",
      "Definition :  formally present a debutante, a representative of a country, etc.\n",
      "Lemmas :  ['present']\n",
      "##### Synset name :  salute.v.06 #####\n",
      "POS :  verb.communication\n",
      "Definition :  recognize with a gesture prescribed by a military regulation; assume a prescribed position\n",
      "Lemmas :  ['salute', 'present']\n",
      "##### Synset name :  present.a.01 #####\n",
      "POS :  adj.all\n",
      "Definition :  temporal sense; intermediate between past and future; now existing or happening or in consideration\n",
      "Lemmas :  ['present']\n",
      "##### Synset name :  present.a.02 #####\n",
      "POS :  adj.all\n",
      "Definition :  being or existing in a specified place\n",
      "Lemmas :  ['present']\n"
     ]
    }
   ],
   "source": [
    "for synset in synsets:\n",
    "    print(\"##### Synset name : \", synset.name(), \"#####\")\n",
    "    print(\"POS : \", synset.lexname())\n",
    "    print(\"Definition : \", synset.definition())\n",
    "    print(\"Lemmas : \", synset.lemma_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordNet은 어떤 어휘와 다른 어휘 간의 관계를 유사도로 나타낼 수 있음.  \n",
    "synset 객체는 단어 간의 유사도를 나타내기 위해 path_similarity() 메서드를 제공함.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tree</th>\n",
       "      <th>lion</th>\n",
       "      <th>tiger</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>tree</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>lion</td>\n",
       "      <td>0.07</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>tiger</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>cat</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>dog</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tree  lion  tiger   cat   dog\n",
       "tree   1.00  0.07   0.07  0.08  0.12\n",
       "lion   0.07  1.00   0.33  0.25  0.17\n",
       "tiger  0.07  0.33   1.00  0.25  0.17\n",
       "cat    0.08  0.25   0.25  1.00  0.20\n",
       "dog    0.12  0.17   0.17  0.20  1.00"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tree, lion, tiger, cat, dog 단어의 상호 유사도 확인\n",
    "# synset 객체를 단어별로 생성함. \n",
    "tree = wn.synset('tree.n.01')\n",
    "lion = wn.synset('lion.n.01')\n",
    "tiger = wn.synset('tiger.n.02')\n",
    "cat = wn.synset('cat.n.01')\n",
    "dog = wn.synset('dog.n.01')\n",
    "\n",
    "entities = [tree, lion, tiger, cat, dog]\n",
    "similarities = []\n",
    "entity_names = [entity.name().split('.')[0] for entity in entities]\n",
    "\n",
    "# 단어별 synset을 반복하면서 다른 단어의 synset과 유사도를 측정합니다.\n",
    "for entity in entities:\n",
    "    similarity = [round(entity.path_similarity(compared_entity), 2) for compared_entity in entities]\n",
    "    similarities.append(similarity)\n",
    "    \n",
    "# 개별 단어별 synset과 다른 단어의 synset과의 유사도를 DataFrame 형태로 저장합니다.\n",
    "similarity_df = pd.DataFrame(similarities, columns=entity_names, index=entity_names)\n",
    "similarity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "senti_synsets() 반환 type :  <class 'list'>\n",
      "senti_synsets() 반환 값 개수 :  11\n",
      "senti_synsets() 반환 값 :  [SentiSynset('decelerate.v.01'), SentiSynset('slow.v.02'), SentiSynset('slow.v.03'), SentiSynset('slow.a.01'), SentiSynset('slow.a.02'), SentiSynset('dense.s.04'), SentiSynset('slow.a.04'), SentiSynset('boring.s.01'), SentiSynset('dull.s.08'), SentiSynset('slowly.r.01'), SentiSynset('behind.r.03')]\n"
     ]
    }
   ],
   "source": [
    "# SentiWordNet은 Senti_Synset 클래스를 가지고 있고 이는 Wordnet의 synset과 유사함.\n",
    "import nltk\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "senti_synsets = list(swn.senti_synsets('slow'))\n",
    "print(\"senti_synsets() 반환 type : \", type(senti_synsets))\n",
    "print(\"senti_synsets() 반환 값 개수 : \", len(senti_synsets))\n",
    "print(\"senti_synsets() 반환 값 : \", senti_synsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SentiSynset 객체는 단어의 감성을 나타내는 **감성 지수**와 객관성(감성과 반대)을 나타내는 **객관성 지수**를 가지고 있음.  \n",
    "감성지수는 **긍정 감성 지수**와 **부정 감성 지수**로 나뉨.  \n",
    "\n",
    "<br>\n",
    "\n",
    "어떤 단어가 전혀 감성적이지 않으면 객관성 지수는 1이 되고, 감성 지수는 모두 0이 됨.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "father 긍정감성 지수 :  0.0\n",
      "father 부정감성 지수 :  0.0\n",
      "father 객관성 지수 :  1.0\n",
      "\n",
      "\n",
      "fabulous 긍정감성 지수 :  0.875\n",
      "fabulous 부정감성 지수 :  0.125\n",
      "fabulous 객관성 지수 :  0.0\n"
     ]
    }
   ],
   "source": [
    "# father(아버지)라는 단어와 fabulous(아주 멋진)라는 두 개 단어의 감성 지수와 객관성 지수\n",
    "import nltk\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "father = swn.senti_synset(\"father.n.01\")\n",
    "print(\"father 긍정감성 지수 : \", father.pos_score())\n",
    "print(\"father 부정감성 지수 : \", father.neg_score())\n",
    "print(\"father 객관성 지수 : \", father.obj_score())\n",
    "print(\"\\n\")\n",
    "\n",
    "fabulous = swn.senti_synset(\"fabulous.a.01\")\n",
    "print(\"fabulous 긍정감성 지수 : \", fabulous.pos_score())\n",
    "print(\"fabulous 부정감성 지수 : \", fabulous.neg_score())\n",
    "print(\"fabulous 객관성 지수 : \", fabulous.obj_score())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SentiWordNet을 이용한 영화 감상평 감성 분석  \n",
    "IMDB 영화 감상평 감성 분석을 SentiWordNet Lexicon 기반으로 수행해 보기.  \n",
    "[SentiWordNet을 이용해 감성 분석 수행하기]  \n",
    "    1. 문서(Document)를 문장(Sentence) 단위로 분해  \n",
    "    2. 다시 문장을 단어(Word) 단위로 토큰화하고 품사 태깅  \n",
    "    3. 품사 태깅된 단어 기반으로 synset 객체와 senti_synset 객체를 생성  \n",
    "    4. Senti_synset에서 긍정 감성/부정 감성 지수를 구하고 이를 모두 합산해 특정 임계치값 이상일 때 긍정 감성으로, 그렇지 않을 때는 부정 감성으로 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# 간단한 NLTK PennTreebank Tag를 기반으로 WordNet 기반의 품사 Tag로 변환\n",
    "def penn_to_wn(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "\n",
    "def swn_polarity(text):\n",
    "    # 감성 지수 초기화\n",
    "    sentiment = 0.0\n",
    "    tokens_count = 0\n",
    "    \n",
    "    # lemmatizer : 원형 복원\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # sent_tokenize() : 문장 토큰화\n",
    "    raw_sentences = sent_tokenize(text)\n",
    "    # 분해된 문장별로 단어 토큰 -> 품사 태깅 후에 SentiSynset 생성 -> 감성 지수 합산\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # NLTK 기반의 품사 태깅 문장 추출\n",
    "        # pos_tag : 단어 토큰에 품사를 부착하여 튜플로 변환\n",
    "        # word_tokenize() : 단어 토큰화\n",
    "        tagged_sentence = pos_tag(word_tokenize(raw_sentence))\n",
    "        for word, tag in tagged_sentence:\n",
    "            # WordNet 기반 품사 태깅과 어근 추출\n",
    "            wn_tag = penn_to_wn(tag)\n",
    "            if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n",
    "                continue\n",
    "            # lammatize : 같은 의미를 가지는 여러 단어를 사전형으로 통일하는 작업(원형으로 변환)\n",
    "            # pos=wn_tag : 품사를 지정\n",
    "            lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "            if not lemma:\n",
    "                continue\n",
    "            # 어근을 추출한 단어와 WordNet 기반 품사 태깅을 입력해 Synset 객체를 생성.\n",
    "            synsets = wn.synsets(lemma, pos=wn_tag)\n",
    "            if not synsets:\n",
    "                continue\n",
    "            # sentiwordnet의 감성 단어 분석으로 감성 synset 추출\n",
    "            # 모든 단어에 대해 긍정 감성 지수는 +로 부정 감성 지수는 -로 합산해 감성 지수 계산.\n",
    "            synset = synsets[0]\n",
    "            swn_synset = swn.senti_synset(synset.name())\n",
    "            sentiment += (swn_synset.pos_score() - swn_synset.neg_score())\n",
    "            tokens_count += 1\n",
    "            \n",
    "    if not tokens_count:\n",
    "            return 0\n",
    "        \n",
    "        # 총 score가 0 이상일 경우 긍정(Positivve) 1, 그렇지 않을 경우 부정(Negative) 0 반환\n",
    "    if sentiment >= 0:\n",
    "            return 1\n",
    "        \n",
    "    return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3724</td>\n",
       "      <td>This version moved a little slow for my taste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23599</td>\n",
       "      <td>I really enjoyed this film because I have a t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11331</td>\n",
       "      <td>Saw this in the theater in     and fell out o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15745</td>\n",
       "      <td>Recently I was looking for the newly issued W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>845</td>\n",
       "      <td>Escaping the life of being pimped by her fath...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6955</td>\n",
       "      <td>This is a generally nice film  with good stor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7653</td>\n",
       "      <td>The real shame of   The Gathering   is not in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9634</td>\n",
       "      <td>In what could have been an otherwise run of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6860</td>\n",
       "      <td>Excellent P O W  adventure  adapted by Eric W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24108</td>\n",
       "      <td>This one features all the  bad  effect of Pri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17500 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review\n",
       "3724    This version moved a little slow for my taste...\n",
       "23599   I really enjoyed this film because I have a t...\n",
       "11331   Saw this in the theater in     and fell out o...\n",
       "15745   Recently I was looking for the newly issued W...\n",
       "845     Escaping the life of being pimped by her fath...\n",
       "...                                                  ...\n",
       "6955    This is a generally nice film  with good stor...\n",
       "7653    The real shame of   The Gathering   is not in...\n",
       "9634    In what could have been an otherwise run of t...\n",
       "6860    Excellent P O W  adventure  adapted by Eric W...\n",
       "24108   This one features all the  bad  effect of Pri...\n",
       "\n",
       "[17500 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# swn_polarity() 함수를 IMDGB 감상평의 개별 문서에 적용\n",
    "review_df['preds'] = review_df['review'].apply(lambda x : swn_polarity(x))\n",
    "y_target = review_df['sentiment'].values\n",
    "preds = review_df['preds'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7668 4832]\n",
      " [3636 8864]]\n",
      "정확도 :  0.6613\n",
      "정밀도 :  0.6472\n",
      "재현율 :  0.7091\n"
     ]
    }
   ],
   "source": [
    "# SentiWordNet의 감성 분석 예측 성능 확인\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score\n",
    "from sklearn.metrics import recall_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "print(confusion_matrix(y_target, preds))\n",
    "print(\"정확도 : \", np.round(accuracy_score(y_target, preds), 4))\n",
    "print(\"정밀도 : \", np.round(precision_score(y_target, preds), 4))\n",
    "print(\"재현율 : \", np.around(recall_score(y_target, preds), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VADER를 이용한 감성 분석  \n",
    "1. 소셜 미디어의 감성 분석 용도로 만들어진 룰 기반의 Lexicon. \n",
    "2. VADER는 SentimentIntersityAnalyzer 클래스를 이용해 쉽게 감성 분석을 제공함.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.13, 'neu': 0.743, 'pos': 0.127, 'compound': -0.7943}\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "senti_analyzer = SentimentIntensityAnalyzer()\n",
    "# polarity_scores() : 감성 점수를 구하는 메서드.\n",
    "senti_scores = senti_analyzer.polarity_scores(review_df['review'][0])\n",
    "print(senti_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- neg : 부정적 감성 지수.  \n",
    "- neu : 중립적인 감성 지수.  \n",
    "- pos : 긍정 감성 지수.  \n",
    "- compound : neg, new, pos score를 적절히 조합해 -1에서 1 사이의 감성 지수를 표현한 값.  \n",
    "\n",
    "compound score를 기반으로 부정 감성, 긍정 감성 여부를 결정함. 보통 0.1 이상이면 긍정 감성임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6736  5764]\n",
      " [ 1867 10633]]\n",
      "정확도 :  0.6948\n",
      "정밀도 :  0.6485\n",
      "재현율 :  0.8506\n"
     ]
    }
   ],
   "source": [
    "# VADER를 이용한 IMDB 감성 분석을 위한 함수인 vader_polarity() 생성\n",
    "def vader_polarity(review, threshold=0.1):\n",
    "    # threshold는 임곗값으로 임곗값보다 크면 긍정, 아니면 부정.\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    scores = analyzer.polarity_scores(review)\n",
    "    \n",
    "    # compound값에 기반해 threshold 입력값보다 크면 1, 그렇지 않으면 0을 반환\n",
    "    agg_score = scores['compound']\n",
    "    final_sentiment = 1 if agg_score >= threshold else 0\n",
    "    return final_sentiment\n",
    "\n",
    "# apply lambda 식을 이용해 레코드별로 vader_polarity()를 수행하고 결과를 'vader_preds'에 저장\n",
    "review_df['vader_preds'] = review_df['review'].apply(lambda x : vader_polarity(x, 0.1))\n",
    "y_target = review_df['sentiment'].values\n",
    "vader_preds = review_df['vader_preds'].values\n",
    "\n",
    "print(confusion_matrix(y_target, vader_preds))\n",
    "print(\"정확도 : \", np.round(accuracy_score(y_target, vader_preds), 4))\n",
    "print(\"정밀도 : \", np.round(precision_score(y_target, vader_preds), 4))\n",
    "print(\"재현율 : \", np.round(recall_score(y_target, vader_preds), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토픽 모델링(Topic Modeling) - 20 뉴스그룹  \n",
    "1. 토픽 모델링(Topic Modeling)  \n",
    "    * 문서 집합에 숨어 있는 주제를 찾아내는 것.  \n",
    "    * 머신러닝 기반의 토픽 모델링을 적용해 숨어 있는 중요 주제를 효과적으로 찾아낼 수 있음.  \n",
    "    * 숨겨진 주제를 효과적으로 표현할 수 있는 중심 단어를 함축적으로 추출함.  \n",
    "2. 머신러닝 기반의 토픽 모델링에 자주 사용되는 기법은 LSA(Latent Semantic Analysis)와 LDA(Latent Dirichlet Allocation)이 있음.  \n",
    "3. 사이킷런은 LDA 기반의 토픽 모델링을 LatentDirichletAllocation 클래스로 제공함.  \n",
    "4. LDA는 Count기반의 벡터화만 사용함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer Shape :  (7862, 1000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# 모토사이클, 야구, 그래픽스, 윈도우즈, 중동, 기독교, 전자공학 의학 8개 주제를 추출.\n",
    "cats = ['rec.motorcycles', 'rec.sport.baseball', 'comp.graphics', 'comp.windows.x', 'talk.politics.mideast', \n",
    "        'soc.religion.christian' ,'sci.electronics', 'sci.med']\n",
    "\n",
    "# 위에서 cats 변수로 기재된 카테고리만 추출. fetch_20newsgropus()의 categories에 cats 입력\n",
    "news_df = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'), categories=cats, random_state=0)\n",
    "\n",
    "# LDA는 Count기반의 벡터화만 적용합니다.\n",
    "count_vect = CountVectorizer(max_df=0.95, max_features=1000, min_df=2, stop_words='english', ngram_range=(1, 2))\n",
    "feat_vect = count_vect.fit_transform(news_df.data)\n",
    "print(\"CountVectorizer Shape : \", feat_vect.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='batch', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=10,\n",
       "                          mean_change_tol=0.001, n_components=8, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=0, topic_word_prior=None,\n",
       "                          total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 피처 벡터화된 데이터셋을 기반으로 LDA 토픽 모델링 수행\n",
    "# n_components : 토픽 개수 조정\n",
    "lda = LatentDirichletAllocation(n_components=8, random_state=0)\n",
    "lda.fit(feat_vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LatentDirichletAllocation.fit()을 수행하면 LatentDirichletAllocation 객체는 **components_ 속성값**을 가짐.  \n",
    "<br>\n",
    "\n",
    "[components_ 속성]  \n",
    "    1. components_ 는 개별 토픽별로 각 word 피처가 얼마나 많이 그 토픽에 할당됐는지에 대한 수치를 가지고 있음.  \n",
    "    2. 높은 값일수록 해당 word 피처는 그 토픽의 중심 word가 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 1000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3.60992018e+01, 1.35626798e+02, 2.15751867e+01, ...,\n",
       "        3.02911688e+01, 8.66830093e+01, 6.79285199e+01],\n",
       "       [1.25199920e-01, 1.44401815e+01, 1.25045596e-01, ...,\n",
       "        1.81506995e+02, 1.25097844e-01, 9.39593286e+01],\n",
       "       [3.34762663e+02, 1.25176265e-01, 1.46743299e+02, ...,\n",
       "        1.25105772e-01, 3.63689741e+01, 1.25025218e-01],\n",
       "       ...,\n",
       "       [3.60204965e+01, 2.08640688e+01, 4.29606813e+00, ...,\n",
       "        1.45056650e+01, 8.33854413e+00, 1.55690009e+01],\n",
       "       [1.25128711e-01, 1.25247756e-01, 1.25005143e-01, ...,\n",
       "        9.17278769e+01, 1.25177668e-01, 3.74575887e+01],\n",
       "       [5.49258690e+01, 4.47009532e+00, 9.88524814e+00, ...,\n",
       "        4.87048440e+01, 1.25034678e-01, 1.25074632e-01]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(lda.components_.shape)\n",
    "lda.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic # 0\n",
      "year 10 game medical health team 12 20 disease cancer 1993 games years patients good\n",
      "Topic # 1\n",
      "don just like know people said think time ve didn right going say ll way\n",
      "Topic # 2\n",
      "image file jpeg program gif images output format files color entry 00 use bit 03\n",
      "Topic # 3\n",
      "like know don think use does just good time book read information people used post\n",
      "Topic # 4\n",
      "armenian israel armenians jews turkish people israeli jewish government war dos dos turkey arab armenia 000\n",
      "Topic # 5\n",
      "edu com available graphics ftp data pub motif mail widget software mit information version sun\n",
      "Topic # 6\n",
      "god people jesus church believe christ does christian say think christians bible faith sin life\n",
      "Topic # 7\n",
      "use dos thanks windows using window does display help like problem server need know run\n"
     ]
    }
   ],
   "source": [
    "# 토픽별로 연관도가 높은 순으로 Word를 나열하는 display_topics() 함수 만들기\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_index, topic in enumerate(model.components_):\n",
    "        print(\"Topic #\", topic_index)\n",
    "        \n",
    "        # components_ array에서 가장 값이 큰 순으로 정렬했을 때, 그 값의 array 인덱스를 반환.\n",
    "        # argsort() : 작은값부터 큰값 순으로 데이터의 위치를 반환\n",
    "        topic_word_indexes = topic.argsort()[::-1]\n",
    "        top_indexes = topic_word_indexes[:no_top_words]\n",
    "        \n",
    "        # top_indexes 대상인 인덱스별로 feature_names에 해당하는 word feature 추출 후 join으로 concat\n",
    "        feature_concat = ' '.join([feature_names[i] for i in top_indexes])\n",
    "        print(feature_concat)\n",
    "        \n",
    "# CountVectorizer 객체 내의 전체 word의 명칭을 get_feature_names()를 통해 추출\n",
    "feature_names = count_vect.get_feature_names()\n",
    "\n",
    "# 토픽별 가장 연관도가 높은 word를 15개만 추출\n",
    "display_topics(lda, feature_names, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문서 군집화 소개와 실습(Opinion Review 데이터 세트)  \n",
    "### 문서 군집화 개념  \n",
    "1. 문서 군집화(Document Clustering)는 비슷한 텍스트 구성의 문서를 군집화(Clustering)하는 것.  \n",
    "2. 텍스트 분류 기반의 문서 분류와 문서 군집화의 차이  \n",
    "    - 텍스트 분류 기반은 학습 데이터 세트가 필요함.  \n",
    "    - 문서 군집화의 경우 비지도학습 기반으로 동작.  \n",
    "    \n",
    "<br>\n",
    "\n",
    "### Opinion Review 데이터 세트를 이용한 문서 군집화 수행하기  \n",
    "* Opinion Review 데이터셋 소개  \n",
    "    1. 51개의 텍스트 파일로 구성.  \n",
    "    2. 각 파일은 Tripadvisor(호텔), Edmunds.com(자동차), Amazon.com(전자제품) 사이트에서 가져온 리뷰 문서임.  \n",
    "    3. 각 100개 정도의 문장을 가지고 있음.  \n",
    "    \n",
    "https://archive.ics.uci.edu/ml/datasets/Opinosis+Opinion+%26frasl%3B+Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>opinion_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>accuracy_garmin_nuvi_255W_gps</td>\n",
       "      <td>, and is very, very acc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>bathroom_bestwestern_hotel_sfo</td>\n",
       "      <td>The room was not overly big, but clean and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>battery-life_amazon_kindle</td>\n",
       "      <td>After I plugged it in to my USB hub on my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>battery-life_ipod_nano_8gb</td>\n",
       "      <td>short battery life  I moved up from a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>battery-life_netbook_1005ha</td>\n",
       "      <td>6GHz 533FSB cpu, glossy display, 3, Cell 2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         filename  \\\n",
       "0   accuracy_garmin_nuvi_255W_gps   \n",
       "1  bathroom_bestwestern_hotel_sfo   \n",
       "2      battery-life_amazon_kindle   \n",
       "3      battery-life_ipod_nano_8gb   \n",
       "4     battery-life_netbook_1005ha   \n",
       "\n",
       "                                        opinion_text  \n",
       "0                         , and is very, very acc...  \n",
       "1      The room was not overly big, but clean and...  \n",
       "2      After I plugged it in to my USB hub on my ...  \n",
       "3           short battery life  I moved up from a...  \n",
       "4      6GHz 533FSB cpu, glossy display, 3, Cell 2...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob, os\n",
    "\n",
    "# 각자 디렉터리를 설정\n",
    "path = r'C:\\Users\\eh063\\Desktop\\은하\\BOAZ\\분석 BASE\\ML\\파이썬 머신러닝 완벽 가이드\\Chapter8\\OpinosisDataset1.0\\topics'\n",
    "# path로 지정한 디렉터리 밑에 있는 모든 .data 파일의 파일명을 리스트로 취합.\n",
    "all_files = glob.glob(os.path.join(path, \"*.data\"))\n",
    "filename_list = []\n",
    "opinion_text = []\n",
    "\n",
    "# 개별 파일의 파일명은 filename_list로 취합, \n",
    "# 개별 파일의 파일 내용은 DataFrame 로딩 후 다시 string으로 변환해 opinion_text list로 취합\n",
    "for file_ in all_files:\n",
    "    # 개별 파일을 읽어서 DataFrame으로 생성\n",
    "    # pd.read_table() : 데이터 파일 읽기\n",
    "    df = pd.read_table(file_, index_col=None, header=0, encoding='latin1')\n",
    "    \n",
    "    # 절대 경로로 주어진 파일명을 가공. 리눅스에서 수행할 때는 다음 \\\\을 /로 변경.\n",
    "    # 맨 마지막 .data 확장자도 제거\n",
    "    \n",
    "    filename_ = file_.split('\\\\')[-1]\n",
    "    filename = filename_.split('.')[0]\n",
    "    \n",
    "    # 파일명 list의 파일 내용 list에 파일명과 파일 내용을 추가.\n",
    "    filename_list.append(filename)\n",
    "    opinion_text.append(df.to_string())\n",
    "    \n",
    "# 파일명 list와 파일 내용 list 객체를 DataFrame으로 생성\n",
    "document_df = pd.DataFrame({'filename':filename_list, 'opinion_text':opinion_text})\n",
    "document_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "lemmar = WordNetLemmatizer()\n",
    "\n",
    "def LemTokens(tokens):\n",
    "    return [lemmar.lemmatize(token) for token in tokens]\n",
    "\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "# 문서를 TF-IDF 형태로 피처 벡터화.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english', ngram_range=(1, 2), min_df=0.05, max_df=0.85)\n",
    "# opinion_text 칼럼 값으로 피처 벡터화 수행\n",
    "feature_vect = tfidf_vect.fit_transform(document_df['opinion_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMeans를 이용하여 5개 집합으로 군집화 수행.\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# 5개 집합으로 군집화 수행.\n",
    "km_cluster = KMeans(n_clusters=5, max_iter=10000, random_state=0)\n",
    "km_cluster.fit(feature_vect)\n",
    "cluster_label = km_cluster.labels_\n",
    "cluster_centers = km_cluster.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>opinion_text</th>\n",
       "      <th>cluster_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>accuracy_garmin_nuvi_255W_gps</td>\n",
       "      <td>, and is very, very acc...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>bathroom_bestwestern_hotel_sfo</td>\n",
       "      <td>The room was not overly big, but clean and...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>battery-life_amazon_kindle</td>\n",
       "      <td>After I plugged it in to my USB hub on my ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>battery-life_ipod_nano_8gb</td>\n",
       "      <td>short battery life  I moved up from a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>battery-life_netbook_1005ha</td>\n",
       "      <td>6GHz 533FSB cpu, glossy display, 3, Cell 2...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         filename  \\\n",
       "0   accuracy_garmin_nuvi_255W_gps   \n",
       "1  bathroom_bestwestern_hotel_sfo   \n",
       "2      battery-life_amazon_kindle   \n",
       "3      battery-life_ipod_nano_8gb   \n",
       "4     battery-life_netbook_1005ha   \n",
       "\n",
       "                                        opinion_text  cluster_label  \n",
       "0                         , and is very, very acc...              2  \n",
       "1      The room was not overly big, but clean and...              0  \n",
       "2      After I plugged it in to my USB hub on my ...              1  \n",
       "3           short battery life  I moved up from a...              1  \n",
       "4      6GHz 533FSB cpu, glossy display, 3, Cell 2...              1  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_df['cluster_label'] = cluster_label\n",
    "document_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>opinion_text</th>\n",
       "      <th>cluster_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>bathroom_bestwestern_hotel_sfo</td>\n",
       "      <td>The room was not overly big, but clean and...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>room_holiday_inn_london</td>\n",
       "      <td>We arrived at 23,30 hours and they could n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>rooms_bestwestern_hotel_sfo</td>\n",
       "      <td>Great Location ,  Nice   Rooms ,  Helpless...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>rooms_swissotel_chicago</td>\n",
       "      <td>The Swissotel is one of our favorite hotel...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          filename  \\\n",
       "1   bathroom_bestwestern_hotel_sfo   \n",
       "32         room_holiday_inn_london   \n",
       "30     rooms_bestwestern_hotel_sfo   \n",
       "31         rooms_swissotel_chicago   \n",
       "\n",
       "                                         opinion_text  cluster_label  \n",
       "1       The room was not overly big, but clean and...              0  \n",
       "32      We arrived at 23,30 hours and they could n...              0  \n",
       "30      Great Location ,  Nice   Rooms ,  Helpless...              0  \n",
       "31      The Swissotel is one of our favorite hotel...              0  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_df[document_df['cluster_label']==0].sort_values(by='filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>opinion_text</th>\n",
       "      <th>cluster_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>battery-life_amazon_kindle</td>\n",
       "      <td>After I plugged it in to my USB hub on my ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>battery-life_ipod_nano_8gb</td>\n",
       "      <td>short battery life  I moved up from a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>battery-life_netbook_1005ha</td>\n",
       "      <td>6GHz 533FSB cpu, glossy display, 3, Cell 2...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>keyboard_netbook_1005ha</td>\n",
       "      <td>,  I think the new keyboard rivals the gre...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>performance_netbook_1005ha</td>\n",
       "      <td>The Eee Super Hybrid Engine utility lets u...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>sound_ipod_nano_8gb</td>\n",
       "      <td>headphone jack i got a clear case for it a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>speed_windows7</td>\n",
       "      <td>Windows 7 is quite simply faster, more sta...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       filename  \\\n",
       "2    battery-life_amazon_kindle   \n",
       "3    battery-life_ipod_nano_8gb   \n",
       "4   battery-life_netbook_1005ha   \n",
       "19      keyboard_netbook_1005ha   \n",
       "26   performance_netbook_1005ha   \n",
       "42          sound_ipod_nano_8gb   \n",
       "44               speed_windows7   \n",
       "\n",
       "                                         opinion_text  cluster_label  \n",
       "2       After I plugged it in to my USB hub on my ...              1  \n",
       "3            short battery life  I moved up from a...              1  \n",
       "4       6GHz 533FSB cpu, glossy display, 3, Cell 2...              1  \n",
       "19      ,  I think the new keyboard rivals the gre...              1  \n",
       "26      The Eee Super Hybrid Engine utility lets u...              1  \n",
       "42      headphone jack i got a clear case for it a...              1  \n",
       "44      Windows 7 is quite simply faster, more sta...              1  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_df[document_df['cluster_label']==1].sort_values(by='filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>opinion_text</th>\n",
       "      <th>cluster_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>accuracy_garmin_nuvi_255W_gps</td>\n",
       "      <td>, and is very, very acc...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>buttons_amazon_kindle</td>\n",
       "      <td>I thought it would be fitting to christen ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>directions_garmin_nuvi_255W_gps</td>\n",
       "      <td>You also get upscale features like spoken ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>display_garmin_nuvi_255W_gps</td>\n",
       "      <td>3 quot  widescreen display was a ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>eyesight-issues_amazon_kindle</td>\n",
       "      <td>It feels as easy to read as the K1 but doe...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>features_windows7</td>\n",
       "      <td>I had to uninstall anti, virus and selecte...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>fonts_amazon_kindle</td>\n",
       "      <td>Being able to change the font sizes is aw...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>navigation_amazon_kindle</td>\n",
       "      <td>In fact, the entire navigation structure h...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>satellite_garmin_nuvi_255W_gps</td>\n",
       "      <td>It's fast to acquire satel...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>screen_garmin_nuvi_255W_gps</td>\n",
       "      <td>It is easy to read and when touching the...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>screen_ipod_nano_8gb</td>\n",
       "      <td>As always, the video screen is sharp and b...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>screen_netbook_1005ha</td>\n",
       "      <td>Keep in mind that once you get in a room ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>size_asus_netbook_1005ha</td>\n",
       "      <td>A few other things I'd like to point out i...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>speed_garmin_nuvi_255W_gps</td>\n",
       "      <td>Another feature on the 255w is a display of...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>updates_garmin_nuvi_255W_gps</td>\n",
       "      <td>Another thing to consider was that I paid $...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>video_ipod_nano_8gb</td>\n",
       "      <td>I bought the 8, gig Ipod Nano that has the...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>voice_garmin_nuvi_255W_gps</td>\n",
       "      <td>The voice prompts and maps are wonderful ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           filename  \\\n",
       "0     accuracy_garmin_nuvi_255W_gps   \n",
       "5             buttons_amazon_kindle   \n",
       "8   directions_garmin_nuvi_255W_gps   \n",
       "9      display_garmin_nuvi_255W_gps   \n",
       "10    eyesight-issues_amazon_kindle   \n",
       "11                features_windows7   \n",
       "12              fonts_amazon_kindle   \n",
       "23         navigation_amazon_kindle   \n",
       "33   satellite_garmin_nuvi_255W_gps   \n",
       "34      screen_garmin_nuvi_255W_gps   \n",
       "35             screen_ipod_nano_8gb   \n",
       "36            screen_netbook_1005ha   \n",
       "41         size_asus_netbook_1005ha   \n",
       "43       speed_garmin_nuvi_255W_gps   \n",
       "48     updates_garmin_nuvi_255W_gps   \n",
       "49              video_ipod_nano_8gb   \n",
       "50       voice_garmin_nuvi_255W_gps   \n",
       "\n",
       "                                         opinion_text  cluster_label  \n",
       "0                          , and is very, very acc...              2  \n",
       "5       I thought it would be fitting to christen ...              2  \n",
       "8       You also get upscale features like spoken ...              2  \n",
       "9                3 quot  widescreen display was a ...              2  \n",
       "10      It feels as easy to read as the K1 but doe...              2  \n",
       "11      I had to uninstall anti, virus and selecte...              2  \n",
       "12       Being able to change the font sizes is aw...              2  \n",
       "23      In fact, the entire navigation structure h...              2  \n",
       "33                      It's fast to acquire satel...              2  \n",
       "34        It is easy to read and when touching the...              2  \n",
       "35      As always, the video screen is sharp and b...              2  \n",
       "36       Keep in mind that once you get in a room ...              2  \n",
       "41      A few other things I'd like to point out i...              2  \n",
       "43     Another feature on the 255w is a display of...              2  \n",
       "48     Another thing to consider was that I paid $...              2  \n",
       "49      I bought the 8, gig Ipod Nano that has the...              2  \n",
       "50       The voice prompts and maps are wonderful ...              2  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_df[document_df['cluster_label']==2].sort_values(by='filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>opinion_text</th>\n",
       "      <th>cluster_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>food_holiday_inn_london</td>\n",
       "      <td>The room was packed to capacity with queu...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>food_swissotel_chicago</td>\n",
       "      <td>The food for our event was deli...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>free_bestwestern_hotel_sfo</td>\n",
       "      <td>The wine reception is a great idea as it i...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>location_bestwestern_hotel_sfo</td>\n",
       "      <td>Good Value good location ,  ideal ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>location_holiday_inn_london</td>\n",
       "      <td>Great location for tube and we crammed in...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>parking_bestwestern_hotel_sfo</td>\n",
       "      <td>Parking was expensive but I think this is ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>price_amazon_kindle</td>\n",
       "      <td>If a case was included, as with the Kindle...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>price_holiday_inn_london</td>\n",
       "      <td>All in all, a normal chain hotel on a nice...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>service_bestwestern_hotel_sfo</td>\n",
       "      <td>Both of us having worked in tourism for o...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>service_holiday_inn_london</td>\n",
       "      <td>not customer, oriented hotelvery low servi...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>service_swissotel_hotel_chicago</td>\n",
       "      <td>Mediocre room and service for a very extr...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>staff_bestwestern_hotel_sfo</td>\n",
       "      <td>Staff are friendly and hel...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>staff_swissotel_chicago</td>\n",
       "      <td>The staff at Swissotel were not particula...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           filename  \\\n",
       "13          food_holiday_inn_london   \n",
       "14           food_swissotel_chicago   \n",
       "15       free_bestwestern_hotel_sfo   \n",
       "20   location_bestwestern_hotel_sfo   \n",
       "21      location_holiday_inn_london   \n",
       "24    parking_bestwestern_hotel_sfo   \n",
       "27              price_amazon_kindle   \n",
       "28         price_holiday_inn_london   \n",
       "38    service_bestwestern_hotel_sfo   \n",
       "39       service_holiday_inn_london   \n",
       "40  service_swissotel_hotel_chicago   \n",
       "45      staff_bestwestern_hotel_sfo   \n",
       "46          staff_swissotel_chicago   \n",
       "\n",
       "                                         opinion_text  cluster_label  \n",
       "13       The room was packed to capacity with queu...              3  \n",
       "14                 The food for our event was deli...              3  \n",
       "15      The wine reception is a great idea as it i...              3  \n",
       "20              Good Value good location ,  ideal ...              3  \n",
       "21       Great location for tube and we crammed in...              3  \n",
       "24      Parking was expensive but I think this is ...              3  \n",
       "27      If a case was included, as with the Kindle...              3  \n",
       "28      All in all, a normal chain hotel on a nice...              3  \n",
       "38       Both of us having worked in tourism for o...              3  \n",
       "39      not customer, oriented hotelvery low servi...              3  \n",
       "40       Mediocre room and service for a very extr...              3  \n",
       "45                      Staff are friendly and hel...              3  \n",
       "46       The staff at Swissotel were not particula...              3  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_df[document_df['cluster_label']==3].sort_values(by='filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>opinion_text</th>\n",
       "      <th>cluster_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>comfort_honda_accord_2008</td>\n",
       "      <td>Drivers seat not comfortable, the car its...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>comfort_toyota_camry_2007</td>\n",
       "      <td>Ride seems comfortable and gas mileage fa...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>gas_mileage_toyota_camry_2007</td>\n",
       "      <td>Ride seems comfortable and gas mileage fa...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>interior_honda_accord_2008</td>\n",
       "      <td>I love the new body style and the interior...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>interior_toyota_camry_2007</td>\n",
       "      <td>First of all, the interior has way too ma...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>mileage_honda_accord_2008</td>\n",
       "      <td>It's quiet, get good gas mileage and look...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>performance_honda_accord_2008</td>\n",
       "      <td>Very happy with my 08 Accord, performance i...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>quality_toyota_camry_2007</td>\n",
       "      <td>I previously owned a Toyota 4Runner which ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>seats_honda_accord_2008</td>\n",
       "      <td>Front seats are very uncomfor...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>transmission_toyota_camry_2007</td>\n",
       "      <td>After slowing down, transmission has to b...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          filename  \\\n",
       "6        comfort_honda_accord_2008   \n",
       "7        comfort_toyota_camry_2007   \n",
       "16   gas_mileage_toyota_camry_2007   \n",
       "17      interior_honda_accord_2008   \n",
       "18      interior_toyota_camry_2007   \n",
       "22       mileage_honda_accord_2008   \n",
       "25   performance_honda_accord_2008   \n",
       "29       quality_toyota_camry_2007   \n",
       "37         seats_honda_accord_2008   \n",
       "47  transmission_toyota_camry_2007   \n",
       "\n",
       "                                         opinion_text  cluster_label  \n",
       "6        Drivers seat not comfortable, the car its...              4  \n",
       "7        Ride seems comfortable and gas mileage fa...              4  \n",
       "16       Ride seems comfortable and gas mileage fa...              4  \n",
       "17      I love the new body style and the interior...              4  \n",
       "18       First of all, the interior has way too ma...              4  \n",
       "22       It's quiet, get good gas mileage and look...              4  \n",
       "25     Very happy with my 08 Accord, performance i...              4  \n",
       "29      I previously owned a Toyota 4Runner which ...              4  \n",
       "37                   Front seats are very uncomfor...              4  \n",
       "47       After slowing down, transmission has to b...              4  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_df[document_df['cluster_label']==4].sort_values(by='filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>opinion_text</th>\n",
       "      <th>cluster_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>accuracy_garmin_nuvi_255W_gps</td>\n",
       "      <td>, and is very, very acc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>updates_garmin_nuvi_255W_gps</td>\n",
       "      <td>Another thing to consider was that I paid $...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>speed_windows7</td>\n",
       "      <td>Windows 7 is quite simply faster, more sta...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>speed_garmin_nuvi_255W_gps</td>\n",
       "      <td>Another feature on the 255w is a display of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>sound_ipod_nano_8gb</td>\n",
       "      <td>headphone jack i got a clear case for it a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>size_asus_netbook_1005ha</td>\n",
       "      <td>A few other things I'd like to point out i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>screen_netbook_1005ha</td>\n",
       "      <td>Keep in mind that once you get in a room ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>screen_ipod_nano_8gb</td>\n",
       "      <td>As always, the video screen is sharp and b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>screen_garmin_nuvi_255W_gps</td>\n",
       "      <td>It is easy to read and when touching the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>satellite_garmin_nuvi_255W_gps</td>\n",
       "      <td>It's fast to acquire satel...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>price_amazon_kindle</td>\n",
       "      <td>If a case was included, as with the Kindle...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>performance_netbook_1005ha</td>\n",
       "      <td>The Eee Super Hybrid Engine utility lets u...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>video_ipod_nano_8gb</td>\n",
       "      <td>I bought the 8, gig Ipod Nano that has the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>navigation_amazon_kindle</td>\n",
       "      <td>In fact, the entire navigation structure h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>keyboard_netbook_1005ha</td>\n",
       "      <td>,  I think the new keyboard rivals the gre...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>voice_garmin_nuvi_255W_gps</td>\n",
       "      <td>The voice prompts and maps are wonderful ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>display_garmin_nuvi_255W_gps</td>\n",
       "      <td>3 quot  widescreen display was a ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>battery-life_netbook_1005ha</td>\n",
       "      <td>6GHz 533FSB cpu, glossy display, 3, Cell 2...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>battery-life_ipod_nano_8gb</td>\n",
       "      <td>short battery life  I moved up from a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>battery-life_amazon_kindle</td>\n",
       "      <td>After I plugged it in to my USB hub on my ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>directions_garmin_nuvi_255W_gps</td>\n",
       "      <td>You also get upscale features like spoken ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>eyesight-issues_amazon_kindle</td>\n",
       "      <td>It feels as easy to read as the K1 but doe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>features_windows7</td>\n",
       "      <td>I had to uninstall anti, virus and selecte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>fonts_amazon_kindle</td>\n",
       "      <td>Being able to change the font sizes is aw...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>buttons_amazon_kindle</td>\n",
       "      <td>I thought it would be fitting to christen ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>food_holiday_inn_london</td>\n",
       "      <td>The room was packed to capacity with queu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>service_holiday_inn_london</td>\n",
       "      <td>not customer, oriented hotelvery low servi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>service_bestwestern_hotel_sfo</td>\n",
       "      <td>Both of us having worked in tourism for o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>bathroom_bestwestern_hotel_sfo</td>\n",
       "      <td>The room was not overly big, but clean and...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>food_swissotel_chicago</td>\n",
       "      <td>The food for our event was deli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>location_bestwestern_hotel_sfo</td>\n",
       "      <td>Good Value good location ,  ideal ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>parking_bestwestern_hotel_sfo</td>\n",
       "      <td>Parking was expensive but I think this is ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>free_bestwestern_hotel_sfo</td>\n",
       "      <td>The wine reception is a great idea as it i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>rooms_swissotel_chicago</td>\n",
       "      <td>The Swissotel is one of our favorite hotel...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>rooms_bestwestern_hotel_sfo</td>\n",
       "      <td>Great Location ,  Nice   Rooms ,  Helpless...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>staff_bestwestern_hotel_sfo</td>\n",
       "      <td>Staff are friendly and hel...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>service_swissotel_hotel_chicago</td>\n",
       "      <td>Mediocre room and service for a very extr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>location_holiday_inn_london</td>\n",
       "      <td>Great location for tube and we crammed in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>staff_swissotel_chicago</td>\n",
       "      <td>The staff at Swissotel were not particula...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>room_holiday_inn_london</td>\n",
       "      <td>We arrived at 23,30 hours and they could n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>price_holiday_inn_london</td>\n",
       "      <td>All in all, a normal chain hotel on a nice...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>transmission_toyota_camry_2007</td>\n",
       "      <td>After slowing down, transmission has to b...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>gas_mileage_toyota_camry_2007</td>\n",
       "      <td>Ride seems comfortable and gas mileage fa...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>comfort_honda_accord_2008</td>\n",
       "      <td>Drivers seat not comfortable, the car its...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>comfort_toyota_camry_2007</td>\n",
       "      <td>Ride seems comfortable and gas mileage fa...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>quality_toyota_camry_2007</td>\n",
       "      <td>I previously owned a Toyota 4Runner which ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>mileage_honda_accord_2008</td>\n",
       "      <td>It's quiet, get good gas mileage and look...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>interior_toyota_camry_2007</td>\n",
       "      <td>First of all, the interior has way too ma...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>interior_honda_accord_2008</td>\n",
       "      <td>I love the new body style and the interior...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>seats_honda_accord_2008</td>\n",
       "      <td>Front seats are very uncomfor...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>performance_honda_accord_2008</td>\n",
       "      <td>Very happy with my 08 Accord, performance i...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           filename  \\\n",
       "0     accuracy_garmin_nuvi_255W_gps   \n",
       "48     updates_garmin_nuvi_255W_gps   \n",
       "44                   speed_windows7   \n",
       "43       speed_garmin_nuvi_255W_gps   \n",
       "42              sound_ipod_nano_8gb   \n",
       "41         size_asus_netbook_1005ha   \n",
       "36            screen_netbook_1005ha   \n",
       "35             screen_ipod_nano_8gb   \n",
       "34      screen_garmin_nuvi_255W_gps   \n",
       "33   satellite_garmin_nuvi_255W_gps   \n",
       "27              price_amazon_kindle   \n",
       "26       performance_netbook_1005ha   \n",
       "49              video_ipod_nano_8gb   \n",
       "23         navigation_amazon_kindle   \n",
       "19          keyboard_netbook_1005ha   \n",
       "50       voice_garmin_nuvi_255W_gps   \n",
       "9      display_garmin_nuvi_255W_gps   \n",
       "4       battery-life_netbook_1005ha   \n",
       "3        battery-life_ipod_nano_8gb   \n",
       "2        battery-life_amazon_kindle   \n",
       "8   directions_garmin_nuvi_255W_gps   \n",
       "10    eyesight-issues_amazon_kindle   \n",
       "11                features_windows7   \n",
       "12              fonts_amazon_kindle   \n",
       "5             buttons_amazon_kindle   \n",
       "13          food_holiday_inn_london   \n",
       "39       service_holiday_inn_london   \n",
       "38    service_bestwestern_hotel_sfo   \n",
       "1    bathroom_bestwestern_hotel_sfo   \n",
       "14           food_swissotel_chicago   \n",
       "20   location_bestwestern_hotel_sfo   \n",
       "24    parking_bestwestern_hotel_sfo   \n",
       "15       free_bestwestern_hotel_sfo   \n",
       "31          rooms_swissotel_chicago   \n",
       "30      rooms_bestwestern_hotel_sfo   \n",
       "45      staff_bestwestern_hotel_sfo   \n",
       "40  service_swissotel_hotel_chicago   \n",
       "21      location_holiday_inn_london   \n",
       "46          staff_swissotel_chicago   \n",
       "32          room_holiday_inn_london   \n",
       "28         price_holiday_inn_london   \n",
       "47   transmission_toyota_camry_2007   \n",
       "16    gas_mileage_toyota_camry_2007   \n",
       "6         comfort_honda_accord_2008   \n",
       "7         comfort_toyota_camry_2007   \n",
       "29        quality_toyota_camry_2007   \n",
       "22        mileage_honda_accord_2008   \n",
       "18       interior_toyota_camry_2007   \n",
       "17       interior_honda_accord_2008   \n",
       "37          seats_honda_accord_2008   \n",
       "25    performance_honda_accord_2008   \n",
       "\n",
       "                                         opinion_text  cluster_label  \n",
       "0                          , and is very, very acc...              0  \n",
       "48     Another thing to consider was that I paid $...              0  \n",
       "44      Windows 7 is quite simply faster, more sta...              0  \n",
       "43     Another feature on the 255w is a display of...              0  \n",
       "42      headphone jack i got a clear case for it a...              0  \n",
       "41      A few other things I'd like to point out i...              0  \n",
       "36       Keep in mind that once you get in a room ...              0  \n",
       "35      As always, the video screen is sharp and b...              0  \n",
       "34        It is easy to read and when touching the...              0  \n",
       "33                      It's fast to acquire satel...              0  \n",
       "27      If a case was included, as with the Kindle...              0  \n",
       "26      The Eee Super Hybrid Engine utility lets u...              0  \n",
       "49      I bought the 8, gig Ipod Nano that has the...              0  \n",
       "23      In fact, the entire navigation structure h...              0  \n",
       "19      ,  I think the new keyboard rivals the gre...              0  \n",
       "50       The voice prompts and maps are wonderful ...              0  \n",
       "9                3 quot  widescreen display was a ...              0  \n",
       "4       6GHz 533FSB cpu, glossy display, 3, Cell 2...              0  \n",
       "3            short battery life  I moved up from a...              0  \n",
       "2       After I plugged it in to my USB hub on my ...              0  \n",
       "8       You also get upscale features like spoken ...              0  \n",
       "10      It feels as easy to read as the K1 but doe...              0  \n",
       "11      I had to uninstall anti, virus and selecte...              0  \n",
       "12       Being able to change the font sizes is aw...              0  \n",
       "5       I thought it would be fitting to christen ...              0  \n",
       "13       The room was packed to capacity with queu...              1  \n",
       "39      not customer, oriented hotelvery low servi...              1  \n",
       "38       Both of us having worked in tourism for o...              1  \n",
       "1       The room was not overly big, but clean and...              1  \n",
       "14                 The food for our event was deli...              1  \n",
       "20              Good Value good location ,  ideal ...              1  \n",
       "24      Parking was expensive but I think this is ...              1  \n",
       "15      The wine reception is a great idea as it i...              1  \n",
       "31      The Swissotel is one of our favorite hotel...              1  \n",
       "30      Great Location ,  Nice   Rooms ,  Helpless...              1  \n",
       "45                      Staff are friendly and hel...              1  \n",
       "40       Mediocre room and service for a very extr...              1  \n",
       "21       Great location for tube and we crammed in...              1  \n",
       "46       The staff at Swissotel were not particula...              1  \n",
       "32      We arrived at 23,30 hours and they could n...              1  \n",
       "28      All in all, a normal chain hotel on a nice...              1  \n",
       "47       After slowing down, transmission has to b...              2  \n",
       "16       Ride seems comfortable and gas mileage fa...              2  \n",
       "6        Drivers seat not comfortable, the car its...              2  \n",
       "7        Ride seems comfortable and gas mileage fa...              2  \n",
       "29      I previously owned a Toyota 4Runner which ...              2  \n",
       "22       It's quiet, get good gas mileage and look...              2  \n",
       "18       First of all, the interior has way too ma...              2  \n",
       "17      I love the new body style and the interior...              2  \n",
       "37                   Front seats are very uncomfor...              2  \n",
       "25     Very happy with my 08 Accord, performance i...              2  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# 3개의 집합으로 군집화\n",
    "km_cluster = KMeans(n_clusters=3, max_iter=10000, random_state=0)\n",
    "km_cluster.fit(feature_vect)\n",
    "cluster_label = km_cluster.labels_\n",
    "cluster_centers = km_cluster.cluster_centers_\n",
    "\n",
    "# 소속 군집을 cluster_label 칼럼으로 할당하고 cluster_label 값으로 정렬\n",
    "document_df['cluster_label'] = cluster_label\n",
    "document_df.sort_values(by='cluster_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>opinion_text</th>\n",
       "      <th>cluster_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>accuracy_garmin_nuvi_255W_gps</td>\n",
       "      <td>, and is very, very acc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>battery-life_amazon_kindle</td>\n",
       "      <td>After I plugged it in to my USB hub on my ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>battery-life_ipod_nano_8gb</td>\n",
       "      <td>short battery life  I moved up from a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>battery-life_netbook_1005ha</td>\n",
       "      <td>6GHz 533FSB cpu, glossy display, 3, Cell 2...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>buttons_amazon_kindle</td>\n",
       "      <td>I thought it would be fitting to christen ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>directions_garmin_nuvi_255W_gps</td>\n",
       "      <td>You also get upscale features like spoken ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>display_garmin_nuvi_255W_gps</td>\n",
       "      <td>3 quot  widescreen display was a ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>eyesight-issues_amazon_kindle</td>\n",
       "      <td>It feels as easy to read as the K1 but doe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>features_windows7</td>\n",
       "      <td>I had to uninstall anti, virus and selecte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>fonts_amazon_kindle</td>\n",
       "      <td>Being able to change the font sizes is aw...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>keyboard_netbook_1005ha</td>\n",
       "      <td>,  I think the new keyboard rivals the gre...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>navigation_amazon_kindle</td>\n",
       "      <td>In fact, the entire navigation structure h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>performance_netbook_1005ha</td>\n",
       "      <td>The Eee Super Hybrid Engine utility lets u...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>price_amazon_kindle</td>\n",
       "      <td>If a case was included, as with the Kindle...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>satellite_garmin_nuvi_255W_gps</td>\n",
       "      <td>It's fast to acquire satel...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>screen_garmin_nuvi_255W_gps</td>\n",
       "      <td>It is easy to read and when touching the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>screen_ipod_nano_8gb</td>\n",
       "      <td>As always, the video screen is sharp and b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>screen_netbook_1005ha</td>\n",
       "      <td>Keep in mind that once you get in a room ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>size_asus_netbook_1005ha</td>\n",
       "      <td>A few other things I'd like to point out i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>sound_ipod_nano_8gb</td>\n",
       "      <td>headphone jack i got a clear case for it a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>speed_garmin_nuvi_255W_gps</td>\n",
       "      <td>Another feature on the 255w is a display of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>speed_windows7</td>\n",
       "      <td>Windows 7 is quite simply faster, more sta...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>updates_garmin_nuvi_255W_gps</td>\n",
       "      <td>Another thing to consider was that I paid $...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>video_ipod_nano_8gb</td>\n",
       "      <td>I bought the 8, gig Ipod Nano that has the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>voice_garmin_nuvi_255W_gps</td>\n",
       "      <td>The voice prompts and maps are wonderful ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           filename  \\\n",
       "0     accuracy_garmin_nuvi_255W_gps   \n",
       "2        battery-life_amazon_kindle   \n",
       "3        battery-life_ipod_nano_8gb   \n",
       "4       battery-life_netbook_1005ha   \n",
       "5             buttons_amazon_kindle   \n",
       "8   directions_garmin_nuvi_255W_gps   \n",
       "9      display_garmin_nuvi_255W_gps   \n",
       "10    eyesight-issues_amazon_kindle   \n",
       "11                features_windows7   \n",
       "12              fonts_amazon_kindle   \n",
       "19          keyboard_netbook_1005ha   \n",
       "23         navigation_amazon_kindle   \n",
       "26       performance_netbook_1005ha   \n",
       "27              price_amazon_kindle   \n",
       "33   satellite_garmin_nuvi_255W_gps   \n",
       "34      screen_garmin_nuvi_255W_gps   \n",
       "35             screen_ipod_nano_8gb   \n",
       "36            screen_netbook_1005ha   \n",
       "41         size_asus_netbook_1005ha   \n",
       "42              sound_ipod_nano_8gb   \n",
       "43       speed_garmin_nuvi_255W_gps   \n",
       "44                   speed_windows7   \n",
       "48     updates_garmin_nuvi_255W_gps   \n",
       "49              video_ipod_nano_8gb   \n",
       "50       voice_garmin_nuvi_255W_gps   \n",
       "\n",
       "                                         opinion_text  cluster_label  \n",
       "0                          , and is very, very acc...              0  \n",
       "2       After I plugged it in to my USB hub on my ...              0  \n",
       "3            short battery life  I moved up from a...              0  \n",
       "4       6GHz 533FSB cpu, glossy display, 3, Cell 2...              0  \n",
       "5       I thought it would be fitting to christen ...              0  \n",
       "8       You also get upscale features like spoken ...              0  \n",
       "9                3 quot  widescreen display was a ...              0  \n",
       "10      It feels as easy to read as the K1 but doe...              0  \n",
       "11      I had to uninstall anti, virus and selecte...              0  \n",
       "12       Being able to change the font sizes is aw...              0  \n",
       "19      ,  I think the new keyboard rivals the gre...              0  \n",
       "23      In fact, the entire navigation structure h...              0  \n",
       "26      The Eee Super Hybrid Engine utility lets u...              0  \n",
       "27      If a case was included, as with the Kindle...              0  \n",
       "33                      It's fast to acquire satel...              0  \n",
       "34        It is easy to read and when touching the...              0  \n",
       "35      As always, the video screen is sharp and b...              0  \n",
       "36       Keep in mind that once you get in a room ...              0  \n",
       "41      A few other things I'd like to point out i...              0  \n",
       "42      headphone jack i got a clear case for it a...              0  \n",
       "43     Another feature on the 255w is a display of...              0  \n",
       "44      Windows 7 is quite simply faster, more sta...              0  \n",
       "48     Another thing to consider was that I paid $...              0  \n",
       "49      I bought the 8, gig Ipod Nano that has the...              0  \n",
       "50       The voice prompts and maps are wonderful ...              0  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_df[document_df['cluster_label']==0].sort_values(by='filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>opinion_text</th>\n",
       "      <th>cluster_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>bathroom_bestwestern_hotel_sfo</td>\n",
       "      <td>The room was not overly big, but clean and...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>food_holiday_inn_london</td>\n",
       "      <td>The room was packed to capacity with queu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>food_swissotel_chicago</td>\n",
       "      <td>The food for our event was deli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>free_bestwestern_hotel_sfo</td>\n",
       "      <td>The wine reception is a great idea as it i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>location_bestwestern_hotel_sfo</td>\n",
       "      <td>Good Value good location ,  ideal ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>location_holiday_inn_london</td>\n",
       "      <td>Great location for tube and we crammed in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>parking_bestwestern_hotel_sfo</td>\n",
       "      <td>Parking was expensive but I think this is ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>price_holiday_inn_london</td>\n",
       "      <td>All in all, a normal chain hotel on a nice...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>room_holiday_inn_london</td>\n",
       "      <td>We arrived at 23,30 hours and they could n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>rooms_bestwestern_hotel_sfo</td>\n",
       "      <td>Great Location ,  Nice   Rooms ,  Helpless...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>rooms_swissotel_chicago</td>\n",
       "      <td>The Swissotel is one of our favorite hotel...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>service_bestwestern_hotel_sfo</td>\n",
       "      <td>Both of us having worked in tourism for o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>service_holiday_inn_london</td>\n",
       "      <td>not customer, oriented hotelvery low servi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>service_swissotel_hotel_chicago</td>\n",
       "      <td>Mediocre room and service for a very extr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>staff_bestwestern_hotel_sfo</td>\n",
       "      <td>Staff are friendly and hel...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>staff_swissotel_chicago</td>\n",
       "      <td>The staff at Swissotel were not particula...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           filename  \\\n",
       "1    bathroom_bestwestern_hotel_sfo   \n",
       "13          food_holiday_inn_london   \n",
       "14           food_swissotel_chicago   \n",
       "15       free_bestwestern_hotel_sfo   \n",
       "20   location_bestwestern_hotel_sfo   \n",
       "21      location_holiday_inn_london   \n",
       "24    parking_bestwestern_hotel_sfo   \n",
       "28         price_holiday_inn_london   \n",
       "32          room_holiday_inn_london   \n",
       "30      rooms_bestwestern_hotel_sfo   \n",
       "31          rooms_swissotel_chicago   \n",
       "38    service_bestwestern_hotel_sfo   \n",
       "39       service_holiday_inn_london   \n",
       "40  service_swissotel_hotel_chicago   \n",
       "45      staff_bestwestern_hotel_sfo   \n",
       "46          staff_swissotel_chicago   \n",
       "\n",
       "                                         opinion_text  cluster_label  \n",
       "1       The room was not overly big, but clean and...              1  \n",
       "13       The room was packed to capacity with queu...              1  \n",
       "14                 The food for our event was deli...              1  \n",
       "15      The wine reception is a great idea as it i...              1  \n",
       "20              Good Value good location ,  ideal ...              1  \n",
       "21       Great location for tube and we crammed in...              1  \n",
       "24      Parking was expensive but I think this is ...              1  \n",
       "28      All in all, a normal chain hotel on a nice...              1  \n",
       "32      We arrived at 23,30 hours and they could n...              1  \n",
       "30      Great Location ,  Nice   Rooms ,  Helpless...              1  \n",
       "31      The Swissotel is one of our favorite hotel...              1  \n",
       "38       Both of us having worked in tourism for o...              1  \n",
       "39      not customer, oriented hotelvery low servi...              1  \n",
       "40       Mediocre room and service for a very extr...              1  \n",
       "45                      Staff are friendly and hel...              1  \n",
       "46       The staff at Swissotel were not particula...              1  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_df[document_df['cluster_label']==1].sort_values(by='filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>opinion_text</th>\n",
       "      <th>cluster_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>comfort_honda_accord_2008</td>\n",
       "      <td>Drivers seat not comfortable, the car its...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>comfort_toyota_camry_2007</td>\n",
       "      <td>Ride seems comfortable and gas mileage fa...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>gas_mileage_toyota_camry_2007</td>\n",
       "      <td>Ride seems comfortable and gas mileage fa...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>interior_honda_accord_2008</td>\n",
       "      <td>I love the new body style and the interior...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>interior_toyota_camry_2007</td>\n",
       "      <td>First of all, the interior has way too ma...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>mileage_honda_accord_2008</td>\n",
       "      <td>It's quiet, get good gas mileage and look...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>performance_honda_accord_2008</td>\n",
       "      <td>Very happy with my 08 Accord, performance i...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>quality_toyota_camry_2007</td>\n",
       "      <td>I previously owned a Toyota 4Runner which ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>seats_honda_accord_2008</td>\n",
       "      <td>Front seats are very uncomfor...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>transmission_toyota_camry_2007</td>\n",
       "      <td>After slowing down, transmission has to b...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          filename  \\\n",
       "6        comfort_honda_accord_2008   \n",
       "7        comfort_toyota_camry_2007   \n",
       "16   gas_mileage_toyota_camry_2007   \n",
       "17      interior_honda_accord_2008   \n",
       "18      interior_toyota_camry_2007   \n",
       "22       mileage_honda_accord_2008   \n",
       "25   performance_honda_accord_2008   \n",
       "29       quality_toyota_camry_2007   \n",
       "37         seats_honda_accord_2008   \n",
       "47  transmission_toyota_camry_2007   \n",
       "\n",
       "                                         opinion_text  cluster_label  \n",
       "6        Drivers seat not comfortable, the car its...              2  \n",
       "7        Ride seems comfortable and gas mileage fa...              2  \n",
       "16       Ride seems comfortable and gas mileage fa...              2  \n",
       "17      I love the new body style and the interior...              2  \n",
       "18       First of all, the interior has way too ma...              2  \n",
       "22       It's quiet, get good gas mileage and look...              2  \n",
       "25     Very happy with my 08 Accord, performance i...              2  \n",
       "29      I previously owned a Toyota 4Runner which ...              2  \n",
       "37                   Front seats are very uncomfor...              2  \n",
       "47       After slowing down, transmission has to b...              2  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_df[document_df['cluster_label']==2].sort_values(by='filename')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 군집별 핵심 단어 추출하기  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster_centers shape :  (3, 2409)\n",
      "[[0.01819865 0.         0.         ... 0.         0.         0.00471073]\n",
      " [0.         0.00170335 0.0025537  ... 0.0032582  0.00349413 0.        ]\n",
      " [0.         0.00137309 0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "cluster_centers = km_cluster.cluster_centers_\n",
    "print(\"cluster_centers shape : \", cluster_centers.shape)\n",
    "print(cluster_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 군집별 top n 핵심 단어, 그 단어의 중심 위치 상댓값, 대상 파일명을 반환함.\n",
    "def get_cluster_details(cluster_model, cluster_data, feature_names, clusters_num, top_n_features=10):\n",
    "    cluster_details = {}\n",
    "    \n",
    "    # cluster_centers array의 값이 큰 순으로 정렬된 인덱스 값을 반환\n",
    "    # 군집 중심정(centroid)별 할당된 word 피처들의 거리값이 큰 순으로 값을 구하기 위함.\n",
    "    centroid_feature_ordered_ind = cluster_model.cluster_centers_.argsort()[:, ::-1]\n",
    "    \n",
    "    # 개별 군집별로 반복하면서 핵심 단어, 그 단어의 중심 위치 상댓값, 대상 파일명 입력\n",
    "    for cluster_num in range(clusters_num):\n",
    "        # 개별 군집별 정보를 담을 데이터 초기화.\n",
    "        cluster_details[cluster_num] = {}\n",
    "        cluster_details[cluster_num]['cluster'] = cluster_num\n",
    "        \n",
    "        # cluster_centers_.argsort()[:, :;-1]로 구한 인덱스를 이용해 top n 피처 단어를 구함.\n",
    "        top_feature_indexes = centroid_feature_ordered_ind[cluster_num, :top_n_features]\n",
    "        top_features = [feature_names[ind] for ind in top_feature_indexes]\n",
    "        \n",
    "        # top_feature_indexes를 이용해 해당 피처 단어의 중심 위치 상댓값 구함.\n",
    "        top_feature_values = cluster_model.cluster_centers_[cluster_num, top_feature_indexes].tolist()\n",
    "        \n",
    "        # cluster_details 딕셔너리 객체에 개별 군집별 핵심단어와 중심위치 상댓값, 해당 파일명 입력\n",
    "        cluster_details[cluster_num]['top_features'] = top_features\n",
    "        cluster_details[cluster_num]['top_features_value'] = top_feature_values\n",
    "        filenames = cluster_data[cluster_data['cluster_label'] == cluster_num]['filename']\n",
    "        filenames = filenames.values.tolist()\n",
    "        \n",
    "        cluster_details[cluster_num]['filenames'] = filenames\n",
    "        \n",
    "    return cluster_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cluster_details(cluster_details):\n",
    "    for cluster_num, cluster_detail in cluster_details.items():\n",
    "        print(\"####### Cluster {0}\".format(cluster_num))\n",
    "        print(\"Top features : \", cluster_detail['top_features'])\n",
    "        print(\"Reviews 파일명 : \", cluster_detail['filenames'][:7])\n",
    "        print(\"=================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### Cluster 0\n",
      "Top features :  ['screen', 'battery', 'life', 'battery life', 'keyboard', 'kindle', 'size', 'button', 'easy', 'voice']\n",
      "Reviews 파일명 :  ['accuracy_garmin_nuvi_255W_gps', 'battery-life_amazon_kindle', 'battery-life_ipod_nano_8gb', 'battery-life_netbook_1005ha', 'buttons_amazon_kindle', 'directions_garmin_nuvi_255W_gps', 'display_garmin_nuvi_255W_gps']\n",
      "=================================================\n",
      "####### Cluster 1\n",
      "Top features :  ['room', 'hotel', 'service', 'location', 'staff', 'food', 'clean', 'bathroom', 'parking', 'room wa']\n",
      "Reviews 파일명 :  ['bathroom_bestwestern_hotel_sfo', 'food_holiday_inn_london', 'food_swissotel_chicago', 'free_bestwestern_hotel_sfo', 'location_bestwestern_hotel_sfo', 'location_holiday_inn_london', 'parking_bestwestern_hotel_sfo']\n",
      "=================================================\n",
      "####### Cluster 2\n",
      "Top features :  ['interior', 'seat', 'mileage', 'comfortable', 'car', 'gas', 'transmission', 'gas mileage', 'ride', 'comfort']\n",
      "Reviews 파일명 :  ['comfort_honda_accord_2008', 'comfort_toyota_camry_2007', 'gas_mileage_toyota_camry_2007', 'interior_honda_accord_2008', 'interior_toyota_camry_2007', 'mileage_honda_accord_2008', 'performance_honda_accord_2008']\n",
      "=================================================\n"
     ]
    }
   ],
   "source": [
    "feature_names = tfidf_vect.get_feature_names()\n",
    "cluster_details = get_cluster_details(cluster_model=km_cluster, cluster_data=document_df, \n",
    "                                     feature_names=feature_names, clusters_num=3, top_n_features=10)\n",
    "print_cluster_details(cluster_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문서 유사도  \n",
    "### 문서 유사도 측정 방법 - 코사인 유사도  \n",
    "1. 문서와 문서 간의 유사도 비교는 일반적으로 코사인 유사도(Cosine Similarity)를 사용함.  \n",
    "2. 코사인 유사도는 벡터와 벡터 간의 유사도를 비교할 때 벡터의 상호 방향성이 얼마나 유사한지에 기반함.  \n",
    "3. 따라서 코사인 유사도는 **두 벡터 사이의 사잇각**을 구해 얼마나 유사한지 수치로 적용함.  \n",
    "<img src='https://lh3.googleusercontent.com/proxy/uhPZ4EN0GcOSbXQwsnWSR-Esbu2-DeX0ZGPitquuFmp4DRmGJUJzvkvgAT0RGaH8Z-7f1lhN28XmZJekRE_WfMJldGfBy1zNj3p7jgwgP2gea7be6Q8qGovvHBXcHvQJQ55fAHXts9L2kPTwIgwJ-QtCiBtf1sOWv2YWamtLcFoXaEs'>\n",
    "\n",
    "4. 유사도는 두 벡터의 내적을 총 벡터 크기의 합을 나눈 것.(즉, 내적 결과를 총 벡터 크기로 정규화(L2 Norm)한 것)\n",
    "\n",
    "### 두 벡터 사잇각  \n",
    "<img src='https://images.deepai.org/glossary-terms/cosine-similarity-1007790.jpg'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코사인 유사도를 구하는 cos_similarity() 함수 작성\n",
    "import numpy as np\n",
    "\n",
    "def cos_similarity(v1, v2):\n",
    "    dot_product = np.dot(v1, v2)\n",
    "    l2_norm = (np.sqrt(sum(np.square(v1))) * np.sqrt(sum(np.square(v2))))\n",
    "    similarity = dot_product / l2_norm\n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 18)\n"
     ]
    }
   ],
   "source": [
    "# 3개의 간단한 문서의 유사도 비교\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "doc_list = ['if you take the blue pill, the story ends', \n",
    "            'if you take the red pill, you stay in Wonderland', \n",
    "            'if you take the red pill, I show you how deep the rabbit hole goes']\n",
    "\n",
    "tfidf_vect_simple = TfidfVectorizer()\n",
    "feature_vect_simple = tfidf_vect_simple.fit_transform(doc_list)\n",
    "print(feature_vect_simple.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 1, 문장 2 Cosine 유사도 : 0.402\n"
     ]
    }
   ],
   "source": [
    "# TfidfVectorizer로 transform()한 결과는 희소 행렬이므로 밀집 행렬로 변환.\n",
    "feature_vect_dense = feature_vect_simple.todense()\n",
    "\n",
    "# 첫 번째 문장과 두 번째 문장의 피처 벡터 추출\n",
    "vect1 = np.array(feature_vect_dense[0]).reshape(-1, )\n",
    "vect2 = np.array(feature_vect_dense[1]).reshape(-1, )\n",
    "\n",
    "# 첫 번째 문장과 두 번째 문장의 피처 벡터로 두 개 문장의 코사인 유사도 추출\n",
    "similarity_simple = cos_similarity(vect1, vect2)\n",
    "print(\"문장 1, 문장 2 Cosine 유사도 : {0:.3f}\".format(similarity_simple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 1, 문장 3 Cosine 유사도 : 0.404\n",
      "문장 2, 문장 3 Cosine 유사도 : 0.456\n"
     ]
    }
   ],
   "source": [
    "vect1 = np.array(feature_vect_dense[0]).reshape(-1, )\n",
    "vect3 = np.array(feature_vect_dense[2]).reshape(-1, )\n",
    "similarity_simple = cos_similarity(vect1, vect3)\n",
    "print(\"문장 1, 문장 3 Cosine 유사도 : {0:.3f}\".format(similarity_simple))\n",
    "\n",
    "vect2 = np.array(feature_vect_dense[1]).reshape(-1, )\n",
    "vect3 = np.array(feature_vect_dense[2]).reshape(-1, )\n",
    "similarity_simple = cos_similarity(vect2, vect3)\n",
    "print(\"문장 2, 문장 3 Cosine 유사도 : {0:.3f}\".format(similarity_simple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.40207758 0.40425045]]\n"
     ]
    }
   ],
   "source": [
    "# 사이킷런은 코사인 유사도를 측정하기 위해 sklearn.metrics.pairwise.cosine_similarity API를 제공함.\n",
    "# 사이킷런의 cosine_similarity는 희소행렬, 밀집행렬 모두 가능하며, 행렬, 배열 모두 가능함\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity_simple_pair = cosine_similarity(feature_vect_simple[0], feature_vect_simple)\n",
    "print(similarity_simple_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.40207758 0.40425045]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity_simple_pair = cosine_similarity(feature_vect_simple[0], feature_vect_simple[1:])\n",
    "print(similarity_simple_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.40207758 0.40425045]\n",
      " [0.40207758 1.         0.45647296]\n",
      " [0.40425045 0.45647296 1.        ]]\n",
      "shape: (3, 3)\n"
     ]
    }
   ],
   "source": [
    "similarity_simple_pair = cosine_similarity(feature_vect_simple, feature_vect_simple)\n",
    "print(similarity_simple_pair)\n",
    "print(\"shape:\", similarity_simple_pair.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opinion Review 데이터 세트를 이용한 문서 유사도 측정  \n",
    "opinion revview의 hotel 관련 review의 첫번째 문서와 다른 hotel 관련 리뷰들 간의 문서 유사도 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eh063\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob, os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "path = r'C:\\Users\\eh063\\Desktop\\은하\\BOAZ\\분석 BASE\\ML\\파이썬 머신러닝 완벽 가이드\\Chapter8\\OpinosisDataset1.0\\topics'\n",
    "all_files = glob.glob(os.path.join(path, \"*.data\"))\n",
    "filename_list = []\n",
    "opinion_text = []\n",
    "\n",
    "for file_ in all_files:\n",
    "    df = pd.read_table(file_, index_col=None, header=0, encoding='latin1')\n",
    "    filename_ = file_.split('\\\\')[-1]\n",
    "    filename = filename_.split('.')[0]\n",
    "    filename_list.append(filename)\n",
    "    opinion_text.append(df.to_string())\n",
    "    \n",
    "document_df = pd.DataFrame({'filename' : filename_list, 'opinion_text' : opinion_text})\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english', ngram_range=(1, 2), min_df=0.05, max_df=0.85)\n",
    "feature_vect = tfidf_vect.fit_transform(document_df['opinion_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_cluster = KMeans(n_clusters=3, max_iter=10000, random_state=0)\n",
    "km_cluster.fit(feature_vect)\n",
    "cluster_label = km_cluster.labels_\n",
    "cluster_centers = km_cluster.cluster_centers_\n",
    "document_df['cluster_label'] = cluster_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "호텔로 군집화 된 문서들의 DataFrame Index :  Int64Index([1, 13, 14, 15, 20, 21, 24, 28, 30, 31, 32, 38, 39, 40, 45, 46], dtype='int64')\n",
      "##### 비교 기준 문서명  bathroom_bestwestern_hotel_sfo  와 타 문서 유사도#####\n",
      "[[1.         0.05907195 0.05404862 0.03739629 0.06629355 0.06734556\n",
      "  0.04017338 0.13113702 0.41011101 0.3871916  0.57253197 0.10600704\n",
      "  0.13058128 0.1602411  0.05539602 0.05839754]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# cluster_label=1인 데이터는 호텔로 군집화된 데이터임. DataFrame에서 해당 인덱스를 추출\n",
    "hotel_indexes = document_df[document_df['cluster_label']==1].index\n",
    "print(\"호텔로 군집화 된 문서들의 DataFrame Index : \", hotel_indexes)\n",
    "\n",
    "# 호텔로 군집화된 데이터 중 첫 번째 문서를 추출해 파일명 표시.\n",
    "comparison_docname = document_df.iloc[hotel_indexes[0]]['filename']\n",
    "print(\"##### 비교 기준 문서명 \", comparison_docname, \" 와 타 문서 유사도#####\")\n",
    "\n",
    "# document_df에서 추출한 Index 객체를 feature_vect로 입력해 호텔 군집화된 feature_vect 추출\n",
    "# 이를 이용해 호텔로 군집화된 문서 중 첫번째 문서와 다른 문서 간의 코사인 유사도 측정.\n",
    "similarity_pair = cosine_similarity(feature_vect[hotel_indexes[0]], feature_vect[hotel_indexes])\n",
    "print(similarity_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'bathroom_bestwestern_hotel_sfo')"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 첫 번째 문서의 다른 문서 간의 유사도가 높은 순으로 시각화\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# 첫번째 문서와 타 문서 간 유사도가 큰 순으로 정렬한 인덱스를 추출하되 자기 자신은 제외.\n",
    "sorted_index = similarity_pair.argsort()[:, ::-1]\n",
    "sorted_index = sorted_index[:, 1:]\n",
    "\n",
    "# 유사도가 큰 순으로 hotel_indexes를 추출해 재정렬\n",
    "hotel_sorted_indexes = hotel_indexes[sorted_index.reshape(-1)]\n",
    "\n",
    "# 유사도가 큰 순으로 유사도값을 재정렬하되 자기 자신은 제외\n",
    "hotel_1_sim_value = np.sort(similarity_pair.reshape(-1))[::-1]\n",
    "hotel_1_sim_value = hotel_1_sim_value[1:]\n",
    "\n",
    "# 유사도가 큰 순으로 정렬된 인덱스와 유사도 값을 이용해 파일명과 유사도 값을 막대 그래프로 시각화\n",
    "hotel_1_sim_df = pd.DataFrame()\n",
    "hotel_1_sim_df['filename'] = document_df.iloc[hotel_sorted_indexes]['filename']\n",
    "hotel_1_sim_df['similarity'] = hotel_1_sim_value\n",
    "\n",
    "sns.barplot(x='similarity', y='filename', data=hotel_1_sim_df)\n",
    "plt.title(comparison_docname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한글 텍스트 처리 - 네이버 영화 평점 감성 분석  \n",
    "### 한글 NLP 처리의 어려움  \n",
    "* 한글 언어 처리의 어려움의 주된 원인  \n",
    "    1. 띄어쓰기 : 띄어쓰기를 잘못하면 의미가 왜곡될 수 있음.  \n",
    "    2. 조사 : 다양한 형태의 조사가 존재하고 어떤 단어가 조사인지 파악하기가 어려움.  \n",
    "    \n",
    "### KoNLPy 소개  \n",
    "1. KoNLPy는 파이썬의 대표적인 한글 형태소 패키지.  \n",
    "2. 형태소 : 단어로서 의미를 가지는 최소 단위.  \n",
    "3. 형태소 분석(Morphological analysis)  \n",
    "    1) 말뭉치를 형태소 어근 단위로 쪼갬.  \n",
    "    2) 각 형태소에 품사 태깅(POS tagging)을 부착.  \n",
    "4. KoNLPy는 기존의 C++/Java로 잘 만들어진 한글 형태소 엔진을 파이썬 래퍼 기반으로 재작성한 패키지로 안정성을 유지할 수 있음.  \n",
    "5. 꼬꼬마(Kkma), 한나눔(Hannanum), Komoran, 은전한닢 프로젝트(Mecab), Twitter 5개의 형태소 분석 모듈을 KoNLPy에서 모두 사용할 수 있음.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 로딩  \n",
    "네이버 영화 평점 데이터 : https://github.com/e9t/nsmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                           document  label\n",
       "0   9976970                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                  너무재밓었다그래서보는것을추천한다      0"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ratings_train.txt 파일 로딩\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(\"ratings_train.txt\", sep='\\t')\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "label -> 1 : 긍정, 0 : 부정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    75173\n",
       "1    74827\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150000 entries, 0 to 149999\n",
      "Data columns (total 3 columns):\n",
      "id          150000 non-null int64\n",
      "document    149995 non-null object\n",
      "label       150000 non-null int64\n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.4+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df의 document에 있는 NULL 값은 공백으로 변환.\n",
    "# document에서 문자가 아닌 숫자의 경우 공백으로 ㅂㄴ환\n",
    "\n",
    "import re\n",
    "\n",
    "train_df = train_df.fillna(' ')\n",
    "# 정규 표현식을 이용해 숫자를 공백으로 변경(정규 표현식으로 \\d는 숫자를 의미함)\n",
    "train_df['document'] = train_df['document'].apply(lambda x : re.sub(r\"\\d+\", \" \", x))\n",
    "\n",
    "# 테스트 데이터 세트를 로딩하고 동일하게 Null 및 숫자를 공백으로 변환\n",
    "test_df = pd.read_csv(\"ratings_test.txt\", sep=\"\\t\")\n",
    "test_df = test_df.fillna(' ')\n",
    "test_df['document'] = test_df['document'].apply(lambda x : re.sub(r\"\\d+\", \" \", x))\n",
    "\n",
    "# id 칼럼 삭제 수행\n",
    "train_df.drop('id', axis=1, inplace=True)\n",
    "test_df.drop('id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF 벡터화 전 각 문장을 한글 형태소 분석을 수행하여 형태소 단어로 토큰화.\n",
    "# 한글 형태소 엔진 : SNS 분석에 적합한 Twitter 클래스 이용\n",
    "# Twitter morphs() 메서드 : 입력 인자로 들어온 문장을 형태소 단어 형태로 토큰화해 list 객체로 반환\n",
    "\n",
    "from konlpy.tag import Twitter\n",
    "\n",
    "twitter = Twitter()\n",
    "def tw_tokenizer(text):\n",
    "    # 입력 인자로 들어온 텍스트를 형태소 단어로 토큰화해 리스트 형태로 반환\n",
    "    tokens_ko = twitter.morphs(text)\n",
    "    return tokens_ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF 수행(tokenizer는 tw_tokenizer() 함수를 이용)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Twitter 객체의 morphs() 객체를 이용한 tokenizer를 사용. ngram_range는 (1, 2)\n",
    "tfidf_vect = TfidfVectorizer(tokenizer=tw_tokenizer, ngram_range=(1, 2), min_df=3, max_df=0.9)\n",
    "tfidf_vect.fit(train_df['document'])\n",
    "tfidf_matrix_train = tfidf_vect.transform(train_df['document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로지스틱 회귀를 이용해 감성 분석 분류 수행.\n",
    "lg_clf = LogisticRegression(random_state=0)\n",
    "\n",
    "# 파라미터 C 최적화를 위해 GridSearchCV를 이용.\n",
    "params = { 'C' : [1, 3.5, 4.5, 5.5, 10] }\n",
    "grid_cv = GridSearchCV(gl_clf, param_grid=params, cv=3, scoring='accuracy', verbose=1)\n",
    "grid_cv.fit(tfidf_matrx_train, train_df['label'])\n",
    "print(grid_cv.best_params_, round(grid_cv.best_score_, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 학습 데이터를 적용한 TfidfVectorizer를 이용해 테스트 데이터를 TF-IDF 값으로 피처 변환함.\n",
    "tfidf_matrix_test = tfidf_vect.transform(test_df['document'])\n",
    "\n",
    "# classifier는 GridSearchCV에서 최적 파라미터로 학습된 classifier를 그대로 이용\n",
    "best_estimator = grid_cv.best_estimator_\n",
    "preds = best_estimator.predict(tfidf_matrix_test)\n",
    "\n",
    "print(\"Logistic Regression 정확도 : \", accuracy_score(test_df['lable'], preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 분석 실습 - 캐글 Mercari Price Suggestion Challenge  \n",
    "* 일본의 대형 온라인 쇼핑몰인 Mercari사의 제품에 대해 가격을 예측.  \n",
    "* 제품에 대한 여러 속성 및 제품 설명 등의 텍스트 데이터로 구성됨.  \n",
    "* 데이터 세트 : https://www.kaggle.com/c/mercari-price-suggestion-challenge/data  \n",
    "\n",
    "### 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1482535, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_id</th>\n",
       "      <th>name</th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>category_name</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>item_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>MLB Cincinnati Reds T Shirt Size XL</td>\n",
       "      <td>3</td>\n",
       "      <td>Men/Tops/T-shirts</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>No description yet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Razer BlackWidow Chroma Keyboard</td>\n",
       "      <td>3</td>\n",
       "      <td>Electronics/Computers &amp; Tablets/Components &amp; P...</td>\n",
       "      <td>Razer</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0</td>\n",
       "      <td>This keyboard is in great condition and works ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>AVA-VIV Blouse</td>\n",
       "      <td>1</td>\n",
       "      <td>Women/Tops &amp; Blouses/Blouse</td>\n",
       "      <td>Target</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Adorable top with a hint of lace and a key hol...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_id                                 name  item_condition_id  \\\n",
       "0         0  MLB Cincinnati Reds T Shirt Size XL                  3   \n",
       "1         1     Razer BlackWidow Chroma Keyboard                  3   \n",
       "2         2                       AVA-VIV Blouse                  1   \n",
       "\n",
       "                                       category_name brand_name  price  \\\n",
       "0                                  Men/Tops/T-shirts        NaN   10.0   \n",
       "1  Electronics/Computers & Tablets/Components & P...      Razer   52.0   \n",
       "2                        Women/Tops & Blouses/Blouse     Target   10.0   \n",
       "\n",
       "   shipping                                   item_description  \n",
       "0         1                                 No description yet  \n",
       "1         0  This keyboard is in great condition and works ...  \n",
       "2         1  Adorable top with a hint of lace and a key hol...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "mercari_df = pd.read_csv('C:\\\\Users\\\\eh063\\\\Desktop\\\\은하\\\\BOAZ\\\\분석 BASE\\\\ML\\\\파이썬 머신러닝 완벽 가이드\\\\Chapter8\\\\mercari-price-suggestion-challenge\\\\mercari_train.tsv', sep='\\t')\n",
    "print(mercari_df.shape)\n",
    "mercari_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1482535 entries, 0 to 1482534\n",
      "Data columns (total 8 columns):\n",
      "train_id             1482535 non-null int64\n",
      "name                 1482535 non-null object\n",
      "item_condition_id    1482535 non-null int64\n",
      "category_name        1476208 non-null object\n",
      "brand_name           849853 non-null object\n",
      "price                1482535 non-null float64\n",
      "shipping             1482535 non-null int64\n",
      "item_description     1482531 non-null object\n",
      "dtypes: float64(1), int64(3), object(4)\n",
      "memory usage: 90.5+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(mercari_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1dca0e502c8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Target값인 price의 데이터 분포도 확인\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "y_train_df = mercari_df['price']\n",
    "plt.figure(figsize=(6, 4))\n",
    "# kde=False : 밀집도 그래프 제외\n",
    "sns.distplot(y_train_df, kde=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1dca0f7ed08>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# price 칼럼 로그 변환\n",
    "import numpy as np\n",
    "y_train_df = np.log1p(mercari_df['price'])\n",
    "sns.distplot(y_train_df, kde=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2.397895\n",
       "1    3.970292\n",
       "2    2.397895\n",
       "Name: price, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mercari_df의 price 컬럼을 로그변환한 값으로 변경\n",
    "mercari_df['price'] = np.log1p(mercari_df['price'])\n",
    "mercari_df['price'].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shipping 값 유형 : \n",
      " 0    819435\n",
      "1    663100\n",
      "Name: shipping, dtype: int64\n",
      "Item_condition_id 값 유형 : \n",
      " 1    640549\n",
      "3    432161\n",
      "2    375479\n",
      "4     31962\n",
      "5      2384\n",
      "Name: item_condition_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# shipping(배송비 유무), item_condition_id(판매자가 제공하는 제품 상태) 칼럼 유형 확인\n",
    "print(\"Shipping 값 유형 : \\n\", mercari_df['shipping'].value_counts())\n",
    "print(\"Item_condition_id 값 유형 : \\n\", mercari_df['item_condition_id'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82489"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# item_description(제품 설명)\n",
    "# 제품에 대한 설명이 없는 경우 \"No description yet\" 값으로 되어 있음. 이 값이 얼마나 되는지 확인하기\n",
    "boolean_cond = mercari_df['item_description'] == \"No description yet\"\n",
    "mercari_df[boolean_cond]['item_description'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### feature : category_name  \n",
    "1. category_name은 '/'로 분리된 카테고리를 하나의 문자열로 나타냄.  \n",
    "2. 예를 들어 category_name이 Men/Tops/T-shirts이면 Men은 대분류, Tops는 중분류, T-shirts는 소분류임.  \n",
    "3. 따라서 category_name을 '/'를 기준으로 단어를 토큰화해 각각 별도의 피처로 저장함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대분류 유형 : \n",
      " Women                     664385\n",
      "Beauty                    207828\n",
      "Kids                      171689\n",
      "Electronics               122690\n",
      "Men                        93680\n",
      "Home                       67871\n",
      "Vintage & Collectibles     46530\n",
      "Other                      45351\n",
      "Handmade                   30842\n",
      "Sports & Outdoors          25342\n",
      "Other_Null                  6327\n",
      "Name: cat_dae, dtype: int64\n",
      "중분류 유형 : \n",
      " 114\n",
      "소분류 유형 : \n",
      " 871\n"
     ]
    }
   ],
   "source": [
    "# apply lambda에서 호출되는 대, 중 소 분할 함수 생성, 대, 중 소 값을 리스트로 반환\n",
    "def split_cat(category_name):\n",
    "    try:\n",
    "        return category_name.split('/')\n",
    "    except:\n",
    "        return ['Other_Null', 'Other_Null', 'Other_Null']\n",
    "    \n",
    "# 위의 split_cat()을 apply lambda에서 호출해 대, 중, 소 칼럼을 mercari_df에 생성.\n",
    "# zip()에 * 연산자가 붙으면 압축 해제. 즉 mercari_df에 lambda를 적용해 split_cat() 함수를 적용하여 반환된 list를 압축 해제 시킴.\n",
    "mercari_df['cat_dae'], mercari_df['cat_jung'], mercari_df['cat_so'] = zip(*mercari_df['category_name'].apply(lambda x : split_cat(x))) \n",
    "\n",
    "# 대분류만 값의 유형과 건수를 살펴보고, 중분류 소부류는 값의 유형이 많으므로 분류 개수만 추출\n",
    "print(\"대분류 유형 : \\n\", mercari_df['cat_dae'].value_counts())\n",
    "# nunique() : unique한 value의 개수를 반환\n",
    "print(\"중분류 유형 : \\n\", mercari_df['cat_jung'].nunique())\n",
    "print(\"소분류 유형 : \\n\", mercari_df['cat_so'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_id             0\n",
       "name                 0\n",
       "item_condition_id    0\n",
       "category_name        0\n",
       "brand_name           0\n",
       "price                0\n",
       "shipping             0\n",
       "item_description     0\n",
       "cat_dae              0\n",
       "cat_jung             0\n",
       "cat_so               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# brand_name(브랜드 이름), category_name(카테고리명), item_description(제품 설명) 칼럼의 Null값을 'Other_Null로 변경'\n",
    "mercari_df['brand_name'] = mercari_df['brand_name'].fillna(value='Other_Null')\n",
    "mercari_df['category_name'] = mercari_df['category_name'].fillna(value='Other_Null')\n",
    "mercari_df['item_description'] = mercari_df['item_description'].fillna(value='Other_Null')\n",
    "\n",
    "# 각 칼럼별로 Null값 건수 확인. 모두 0이 나와야 함.\n",
    "mercari_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 피처 인코딩과 피처 벡터화  \n",
    "인코딩할 피처는 모두 원-핫 인코딩을, 비교적 짧은 텍스트의 경우는 Count기반 벡터화를, 긴 텍스트는 TF-IDF 기반의 벡터화 적용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['train_id', 'name', 'item_condition_id', 'category_name', 'brand_name',\n",
       "       'price', 'shipping', 'item_description', 'cat_dae', 'cat_jung',\n",
       "       'cat_so'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mercari_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brand name의 유형 건수 :  4810\n",
      "brand name sample 5건 :  Other_Null           632682\n",
      "PINK                  54088\n",
      "Nike                  54043\n",
      "Victoria's Secret     48036\n",
      "LuLaRoe               31024\n",
      "Name: brand_name, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# brand_name\n",
    "# 상품브랜드명의 유형 건수와 대표적인 브랜드명 5개 출력\n",
    "print(\"brand name의 유형 건수 : \", mercari_df['brand_name'].nunique())\n",
    "print(\"brand name sample 5건 : \", mercari_df['brand_name'].value_counts().head(5))\n",
    "\n",
    "### brand_name 컬럼은 원-핫 인코딩 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name의 종류 개수 :  1225273\n",
      "name sample 10건 :  Bundle                 2232\n",
      "Reserved                453\n",
      "Converse                445\n",
      "BUNDLE                  418\n",
      "Dress                   410\n",
      "Coach purse             404\n",
      "Lularoe TC leggings     396\n",
      "Romper                  353\n",
      "Nike                    340\n",
      "Vans                    334\n",
      "Name: name, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# name\n",
    "# name의 유형 건수와 대표적인 브랜드명 10개 출력\n",
    "print(\"name의 종류 개수 : \", mercari_df['name'].nunique())\n",
    "print(\"name sample 10건 : \", mercari_df['name'].value_counts().head(10))\n",
    "\n",
    "### name 칼럼은 Count 기반 피처 벡터화 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### cat_dae, cat_jung, cat_so, shipping, id는 원-핫 인코딩 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_description 평균 문자열 크기 :  145.7113889385411\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                                                                                                                                                                              No description yet\n",
       "1    This keyboard is in great condition and works like it came out of the box. All of the ports are tested and work perfectly. The lights are customizable via the Razer Synapse app on your PC.\n",
       "Name: item_description, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# item_description\n",
    "pd.set_option('max_colwidth', 200)\n",
    "\n",
    "# item_description의 평균 문자열 크기\n",
    "print(\"item_description 평균 문자열 크기 : \", mercari_df['item_description'].str.len().mean())\n",
    "mercari_df['item_description'][:2]\n",
    "\n",
    "### item_description은 TF-IDF로 변환."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name vectorization shape :  (1482535, 105757)\n",
      "item_description vectorization shape :  (1482535, 50000)\n"
     ]
    }
   ],
   "source": [
    "# name 속성에 대한 피처 벡터화 변환\n",
    "cnt_vec = CountVectorizer()\n",
    "X_name = cnt_vec.fit_transform(mercari_df.name)\n",
    "\n",
    "# item_description에 대한 피처 벡터화 변환\n",
    "tfidf_descp = TfidfVectorizer(max_features=50000, ngram_range=(1, 3), stop_words='english')\n",
    "X_descp = tfidf_descp.fit_transform(mercari_df['item_description'])\n",
    "\n",
    "print(\"name vectorization shape : \", X_name.shape)\n",
    "print(\"item_description vectorization shape : \", X_descp.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "나머지 피처들에 대해 인코딩을 수행해야함.  \n",
    "TF-IDF 벡터화와 Count 벡터화는 모두 희소 행렬을 반환함.  \n",
    "따라서 원-핫 인코딩 또한 희소 행렬로 변환 후 빅처 벡터화된 희소 행렬들과 결합해야 함.  \n",
    "사이킷런은 원-핫 인코딩을 위해 OneHotEncoder와 LabelBinarizer를 제공함. 그 중에서 LabelBinarizer 클래스가 희소 행렬을 지원하므로 LabelBinarizer 클래스를 이용해 원-핫 인코딩 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# brand_name, item_condition_id, shipping 각 피처들을 희소 행렬 원-핫 인코딩 변환\n",
    "# sparse_output=True로 파라미터를 설정해줘야 희소행렬 지원\n",
    "lb_brand_name = LabelBinarizer(sparse_output=True)\n",
    "X_brand = lb_brand_name.fit_transform(mercari_df['brand_name'])\n",
    "lb_item_cond_id = LabelBinarizer(sparse_output=True)\n",
    "X_item_cond_id = lb_item_cond_id.fit_transform(mercari_df['item_condition_id'])\n",
    "lb_shipping = LabelBinarizer(sparse_output=True)\n",
    "X_shipping = lb_shipping.fit_transform(mercari_df['shipping'])\n",
    "\n",
    "# cat_dae, cat_jung, cat_so 각 피처들을 희소 행렬 원-핫 인코딩 변환\n",
    "lb_cat_dae = LabelBinarizer(sparse_output=True)\n",
    "X_cat_dae = lb_cat_dae.fit_transform(mercari_df['cat_dae'])\n",
    "lb_cat_jung = LabelBinarizer(sparse_output=True)\n",
    "X_cat_jung = lb_cat_jung.fit_transform(mercari_df['cat_jung'])\n",
    "lb_cat_so = LabelBinarizer(sparse_output=True)\n",
    "X_cat_so = lb_cat_so.fit_transform(mercari_df['cat_so'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'> <class 'scipy.sparse.csr.csr_matrix'> <class 'scipy.sparse.csr.csr_matrix'>\n",
      "X_brand shape : (1482535, 4810), X_item_cond_id shape : (1482535, 5)\n",
      "X_shipping shape : (1482535, 1), X_cat_dae shape : (1482535, 11)\n",
      "X_cat_jung shape : (1482535, 114), X_cat_so shape : (1482535, 871)\n"
     ]
    }
   ],
   "source": [
    "print(type(X_brand), type(X_item_cond_id), type(X_shipping))\n",
    "print(\"X_brand shape : {0}, X_item_cond_id shape : {1}\".format(X_brand.shape, X_item_cond_id.shape))\n",
    "print(\"X_shipping shape : {0}, X_cat_dae shape : {1}\".format(X_shipping.shape, X_cat_dae.shape))\n",
    "print(\"X_cat_jung shape : {0}, X_cat_so shape : {1}\".format(X_cat_jung.shape, X_cat_so.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'> (1482535, 161569)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hstack()을 이용해 피처 벡터화 데이터셋과 원핫 인코딩 데이터셋을 결합함.\n",
    "from scipy.sparse import hstack\n",
    "import gc\n",
    "\n",
    "sparse_matrix_list = (X_name, X_descp, X_brand, X_item_cond_id, X_shipping, X_cat_dae, X_cat_jung, X_cat_so)\n",
    "\n",
    "# hstack 함수를 이용해 인코딩과 벡터화를 수행한 데이터 세트를 모두 결합.\n",
    "X_features_sparse = hstack(sparse_matrix_list).tocsr()\n",
    "print(type(X_features_sparse), X_features_sparse.shape)\n",
    "\n",
    "# 데이터 세트가 메모리를 많이 차지하므로 사용 목적이 끝났으면 바로 메모리에서 삭제.\n",
    "del X_features_sparse\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 릿지 회귀 모델 구축 및 평가  \n",
    "1. 적용할 평가 지표는 RMSLE(Root Mean Square logarithmic Error).  \n",
    "2. RMSLE는 MLE값에 로그를 취한 값. 높은 가격에서 오류가 발생할 경우 오류값이 매우 커지는 것을 방지하기 위해 RMSLE를 사용.  \n",
    "\n",
    "<img src='https://img1.daumcdn.net/thumb/R800x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FcMpUzP%2FbtqwHdHbYW2%2FhEC2CA3HqqvNl0hrcNBkl1%2Fimg.png'>\n",
    "    \n",
    "    - RMSLE를 구하는 함수인 rmsle(y, y_pred)를 생성하자.  \n",
    "    - 여기서 주의할 점은 예측된 가격은 로그 변환된 예측값이므로 오류값을 구하기 전에 지수화를 통해 예측값을 원본으로 복구시켜야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(y, y_pred):\n",
    "    # underflow, overflow를 막기 위해 log가 아닌 log1p로 rmsle 계산.\n",
    "    # np.power() : 첫번째 인자를 두번째 인자로 거듭제곱 함.\n",
    "    return np.sqrt(np.mean(np.power(np.log1p(y) - np.log1p(y_pred), 2)))\n",
    "\n",
    "def evaluate_org_price(y_test, preds):\n",
    "    # 원본 데이터는 log1p로 변환되었으므로 exmpm1로 원복 필요.\n",
    "    preds_exmpm = np.expm1(preds)\n",
    "    y_test_exmpm = np.expm1(y_test)\n",
    "    \n",
    "    # rmsle로 RMSLE값 추출\n",
    "    rmsle_result = rmsle(y_test_exmpm, preds_exmpm)\n",
    "    return rmsle_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "'''\n",
    "피처벡터화 데이터셋과 인코딩 데이터셋을 결합하고, 훈련 데이터와 테스트 데이터셋으로 나누고, 모델을 생성/예측하여 예측값을 반환하는 \n",
    "model_train_predict() 함수 생성\n",
    "'''\n",
    "def model_train_predict(model, matrix_list):\n",
    "    # scipy.sparse 모듈의 hstack을 이용해 희소 행렬 결합\n",
    "    X = hstack(matrix_list).tocsr()\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, mercari_df['price'], test_size=0.2, random_state=156)\n",
    "    \n",
    "    # 모델 학습 및 예측\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    \n",
    "    # 메모리를 위해 삭제\n",
    "    del X, X_train, X_test, y_train\n",
    "    gc.collect()\n",
    "    \n",
    "    return preds, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item Description을 제외했을 때 rmsle값 :  0.5021632139113013\n",
      "Item Description을 포함한 rmsle값 :  0.47121974508153563\n"
     ]
    }
   ],
   "source": [
    "# Ridge를 이용해 회귀 예측 수행. 단, item_description과 같은 텍스트 형태의 속성이 얼마나 영향을 미치는지 알아보기.\n",
    "linear_model = Ridge(solver='lsqr', fit_intercept=False)\n",
    "\n",
    "sparse_matrix_list = (X_name, X_brand, X_item_cond_id, X_shipping, X_cat_dae, X_cat_jung, X_cat_so)\n",
    "linear_preds, y_test = model_train_predict(linear_model, sparse_matrix_list)\n",
    "print(\"Item Description을 제외했을 때 rmsle값 : \", evaluate_org_price(y_test, linear_preds))\n",
    "\n",
    "sparse_matrix_list = (X_descp, X_name, X_brand, X_item_cond_id, X_shipping, X_cat_dae, X_cat_jung, X_cat_so)\n",
    "linear_preds, y_test = model_train_predict(linear_model, sparse_matrix_list)\n",
    "print(\"Item Description을 포함한 rmsle값 : \", evaluate_org_price(y_test, linear_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM 회귀 모델 구축과 앙상블을 이용한 최종 예측 평가  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM rmsle값 :  0.4571726269143959\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "sparse_matrix_list = (X_descp, X_name, X_brand, X_item_cond_id, X_shipping, X_cat_dae, X_cat_jung, X_cat_so)\n",
    "lgbm_model = LGBMRegressor(n_estimators=200, learning_rate=0.5, num_leaves=125, random_state=156)\n",
    "lgbm_preds, y_test = model_train_predict(lgbm_model, sparse_matrix_list)\n",
    "print(\"LightGBM rmsle값 : \", evaluate_org_price(y_test, lgbm_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM과 Ridge를 ensemble한 최종 rmsle값 :  0.4504770028108561\n"
     ]
    }
   ],
   "source": [
    "# LightGBM의 예측 결괏값과 Ridge의 예측 결괏값을 서로 앙상블해 최종 예측 결괏값을 도출.\n",
    "preds = lgbm_preds*0.45 + linear_preds*0.55\n",
    "print(\"LightGBM과 Ridge를 ensemble한 최종 rmsle값 : \", evaluate_org_price(y_test, preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
